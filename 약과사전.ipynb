{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IY5y39AwUsQl",
        "Obn44F-gSTq6",
        "7g-id5W6SXSP",
        "ZKcgBUiDmCox",
        "Ti_HFujBScjM",
        "12ycQP1BSkQB",
        "zHHUqQdYSomI",
        "jkX4Ty4Nl1Aw",
        "z35f8n_GStH_",
        "vwO2fbIilhWI",
        "ngc63B9q02X3",
        "ODOl0rYH0vjs",
        "RBKlWutuLdn2",
        "7_1wJOZWLp-0",
        "bS-C4L-VMAOh",
        "A2dFtlTuMQdf",
        "xcKWA3NEMYPH",
        "Ijg59q82Mbvb",
        "Odu5HEkVMivM",
        "hzBc2OLvMnpJ",
        "zOfquQnhMs_6",
        "UZol2gyfMyED"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k984530/-url/blob/main/%EC%95%BD%EA%B3%BC%EC%82%AC%EC%A0%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Mount"
      ],
      "metadata": {
        "id": "WOtd5x49SPmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZhgbVvms_vnT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e75ef5fc-34ff-4796-fd6a-b4fa3556a9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**객체 탐지 Model - Yolo.v8m**"
      ],
      "metadata": {
        "id": "wJclE3RYm3eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "66VtD86yn8vZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fbf5b0-2ec0-4c74-f6f0-0d13f65a68bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.176-py3-none-any.whl (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.3/616.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "jiqe63-oA-Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelpath = \"/content/drive/MyDrive/data/1.Training/라벨링데이터/경구약제조합 5000종/TL_1_조합\"\n",
        "imgpath =\"/content/drive/MyDrive/data/1.Training/원천데이터/경구약제조합 5000종/TS_1_조합\""
      ],
      "metadata": {
        "id": "C3DiQ9SQBpPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_json(file):\n",
        "  return file[:-5]"
      ],
      "metadata": {
        "id": "ncdjbfDgfoEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yolo 학습을 위한 파일 준비"
      ],
      "metadata": {
        "id": "IY5y39AwUsQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # #for dir in os.listdir(filepath):\n",
        "\n",
        "# labeldata = os.listdir(labelpath)\n",
        "# train_label_data = labeldata[:int(len(labeldata)*0.75)]\n",
        "# val_label_data = labeldata[int(len(labeldata)*0.75):]\n",
        "# train_img_data = list(map(remove_json,train_label_data))\n",
        "# val_img_data = list(map(remove_json,val_label_data))\n",
        "# img1=\"_0_2_0_2_70_000_200.png\"\n",
        "# img2=\"_0_2_0_2_75_000_200.png\"\n",
        "# img3=\"_0_2_0_2_90_000_200.png\"\n",
        "# imgs = [img1,img2,img3]\n",
        "\n",
        "# # for dir in train_label_data:\n",
        "# #   pre = dir[0]\n",
        "# #   file1 = pre+\"-\"+dir[2:8]\n",
        "# #   file2 = pre+\"-\"+dir[9:15]\n",
        "# #   file3 = pre+\"-\"+dir[16:22]\n",
        "# #   file4 = pre+\"-\"+dir[23:29]\n",
        "# #   path1 = filepath+\"/\"+dir+\"/\"+file1\n",
        "# #   path2 = filepath+\"/\"+dir+\"/\"+file2\n",
        "# #   path3 = filepath+\"/\"+dir+\"/\"+file3\n",
        "# #   path4 = filepath+\"/\"+dir+\"/\"+file4\n",
        "# #   paths = [path1,path2,path3,path4]\n",
        "# #   for p in paths:\n",
        "# #     for jsonfile in glob(p+\"/*.json\"):\n",
        "# #       with open(jsonfile, \"r\") as json_file:\n",
        "# #         data = json.load(json_file)\n",
        "# #       if(len(data['annotations'][0]['bbox'])>3):\n",
        "# #         x = (data['annotations'][0]['bbox'][0]+data['annotations'][0]['bbox'][2]/2)/data['images'][0]['width']\n",
        "# #         w = data['annotations'][0]['bbox'][2]/data['images'][0]['width']\n",
        "# #         y = (data['annotations'][0]['bbox'][1]+data['annotations'][0]['bbox'][3]/2)/data['images'][0]['height']\n",
        "# #         h = data['annotations'][0]['bbox'][3]/data['images'][0]['height']\n",
        "# #       name = data['images'][0]['file_name'][:-4]\n",
        "# #       f = open('/content/drive/MyDrive/Yolov8_Model/data/train/'+ name +'.txt','a')\n",
        "# #       if(len(data['annotations'][0]['bbox'])>3):\n",
        "# #         f.write(\"0 \"+str(x)+\" \"+str(y)+\" \"+str(w)+\" \"+str(h)+\"\\n\")\n",
        "# #       if(len(data['annotations'][0]['bbox'])<=3):\n",
        "# #         f.write(\"\")\n",
        "# #       f.close()\n",
        "\n",
        "# # for dir in val_label_data:\n",
        "# #   pre = dir[0]\n",
        "# #   file1 = pre+\"-\"+dir[2:8]\n",
        "# #   file2 = pre+\"-\"+dir[9:15]\n",
        "# #   file3 = pre+\"-\"+dir[16:22]\n",
        "# #   file4 = pre+\"-\"+dir[23:29]\n",
        "# #   path1 = filepath+\"/\"+dir+\"/\"+file1\n",
        "# #   path2 = filepath+\"/\"+dir+\"/\"+file2\n",
        "# #   path3 = filepath+\"/\"+dir+\"/\"+file3\n",
        "# #   path4 = filepath+\"/\"+dir+\"/\"+file4\n",
        "# #   paths = [path1,path2,path3,path4]\n",
        "# #   for p in paths:\n",
        "# #     for jsonfile in glob(p+\"/*.json\"):\n",
        "# #       with open(jsonfile, \"r\") as json_file:\n",
        "# #         data = json.load(json_file)\n",
        "# #       if(len(data['annotations'][0]['bbox'])>3):\n",
        "# #         x = (data['annotations'][0]['bbox'][0]+data['annotations'][0]['bbox'][2]/2)/data['images'][0]['width']\n",
        "# #         w = data['annotations'][0]['bbox'][2]/data['images'][0]['width']\n",
        "# #         y = (data['annotations'][0]['bbox'][1]+data['annotations'][0]['bbox'][3]/2)/data['images'][0]['height']\n",
        "# #         h = data['annotations'][0]['bbox'][3]/data['images'][0]['height']\n",
        "# #       name = data['images'][0]['file_name'][:-4]\n",
        "# #       f = open('/content/drive/MyDrive/Yolov8_Model/data/val/'+ name +'.txt','a')\n",
        "# #       if(len(data['annotations'][0]['bbox'])>3):\n",
        "# #         f.write(\"0 \"+str(x)+\" \"+str(y)+\" \"+str(w)+\" \"+str(h)+\"\\n\")\n",
        "# #       if(len(data['annotations'][0]['bbox'])<=3):\n",
        "# #         f.write(\"\")\n",
        "# #       f.close()\n",
        "\n",
        "# for dir in train_img_data:\n",
        "#   ip = imgpath + \"/\" + dir +\"/\"+dir\n",
        "#   for img in imgs:\n",
        "#     temp = ip+img\n",
        "#     os.replace(temp,\"/content/drive/MyDrive/Yolov8_Model/data/train/\"+dir+img)\n",
        "\n",
        "# for dir in val_img_data:\n",
        "#   ip = imgpath + \"/\" + dir +\"/\"+dir\n",
        "#   for img in imgs:\n",
        "#     temp = ip+img\n",
        "#     os.replace(temp,\"/content/drive/MyDrive/Yolov8_Model/data/val/\"+dir+img)"
      ],
      "metadata": {
        "id": "QptQh02UBKs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "sQ2wqYnIm8wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "jfYG0crmm8Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Yolo = YOLO(\"/content/drive/MyDrive/runs/detect/train2/weights/best.pt\") # 원하는 크기 모델 입력(n ~ x)\n"
      ],
      "metadata": {
        "id": "y2wxaWVI_6jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**객체 분류 Model - ResNet152**"
      ],
      "metadata": {
        "id": "lPqWWC3VuY81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install pkg"
      ],
      "metadata": {
        "id": "Obn44F-gSTq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PIL error 해결 방안\n",
        "!pip install Pillow\n",
        "\n",
        "!pip install torch torchvision\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install imgaug\n",
        "!pip install PIL\n",
        "!pip install tqdm\n",
        "!pip install codecs\n",
        "!pip install json\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "2-gXpE3drUlD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c717e8-b6ae-4e3b-8e3a-a1184edfc273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.11.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.31.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.0.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (2023.8.30)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (2.8.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement codecs (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for codecs\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import pkg"
      ],
      "metadata": {
        "id": "7g-id5W6SXSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from imgaug import augmenters as iaa\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import datetime\n",
        "import argparse\n",
        "from types import SimpleNamespace\n",
        "import codecs\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "8iAsARsL2_gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageTransform"
      ],
      "metadata": {
        "id": "ZKcgBUiDmCox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTransform() :\n",
        "    def __init__(self, resize, mean, std) :\n",
        "        self.data_transform = {\n",
        "            'train' : transforms.Compose([\n",
        "                transforms.RandomResizedCrop(resize, scale=(0.5, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ]),\n",
        "            'val' : transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(resize),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase) :\n",
        "        return self.data_transform[phase](img)"
      ],
      "metadata": {
        "id": "p38k1AJjud2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable"
      ],
      "metadata": {
        "id": "Ti_HFujBScjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = '/content/drive/MyDrive/data/1.Training/원천데이터/단일경구약제 5000종/TS_81_단일/K-038884/K-038884_0_0_0_0_75_000_200.png'\n",
        "label_folder = '/content/drive/MyDrive/data/1.Training/라벨링데이터/단일경구약제 5000종/K-038884_json/K-038884_0_0_0_0_75_000_200.json'\n",
        "\n",
        "dest_data_folder = '/content/drive/MyDrive/data/1.Training/정제원천데이터/단일경구약제 5000종/TS_81_단일/K-038884/K-038884_0_0_0_0_75_000_200.png'\n",
        "dest_label_folder = '/content/drive/MyDrive/data/1.Training/정제라벨링데이터/단일경구약제 5000종/K-038884_json/K-038884_0_0_0_0_75_000_200.json'\n",
        "\n",
        "# 학습한 파라미터가 저장된 파일 경로\n",
        "trained_params_path = '/content/drive/MyDrive/data/proj_pill/pill_resnet152_dataclass01_aug0.pt'\n",
        "\n",
        "model = None\n",
        "criterion = None\n",
        "optimizer = None\n",
        "epoch_begin = 0\n",
        "log_writer = None\n",
        "\n",
        "\n",
        "job = 'resnet152'\n",
        "\n",
        "size = 224\n",
        "mean = (0.485, 0.456, 0.456)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32\n",
        "\n",
        "###GEN_PILL###\n",
        "\n",
        "list_level_aug_geo = [\n",
        "    [0, iaa.Identity()],\n",
        "]\n",
        "\n",
        "list_level_aug_geo_scale = [\n",
        "    [0, iaa.Identity()],\n",
        "    [1, iaa.Affine(scale=0.85)],\n",
        "    [2, iaa.Affine(scale=0.80)],\n",
        "    [3, iaa.Affine(scale=0.75)],\n",
        "    [3, iaa.Affine(scale=0.70)],\n",
        "]\n",
        "\n",
        "list_level_aug_geo_rotate = [\n",
        "    [0,iaa.Identity()],\n",
        "]\n",
        "\n",
        "list_level_aug_non_geo = [\n",
        "    [0,iaa.Identity()],\n",
        "    [1,iaa.GaussianBlur(sigma=1.0)],\n",
        "    [2,iaa.Dropout(p=0.05)],\n",
        "    [2,iaa.CoarseDropout(0.02, size_percent=0.5)],\n",
        "    [2,iaa.SaltAndPepper(0.1)],\n",
        "    [2,iaa.JpegCompression(compression=10)],\n",
        "    [3,iaa.ChangeColorTemperature(1000)],\n",
        "    [3,iaa.ChangeColorTemperature(4000)],\n",
        "    [3,iaa.Snowflakes(flake_size=0.1, speed=0.01)],\n",
        "    [3,iaa.GammaContrast(2.0)],\n",
        "]\n",
        "\n",
        "###GEN_DIGIT###\n",
        "\n",
        "list_aug_geo = []\n",
        "list_aug_geo_scale=[]\n",
        "list_aug_geo_rotate = []\n",
        "list_aug_non_geo = []\n",
        "\n",
        "\n",
        "###############\n",
        "\n",
        "transform_normalize = transforms.Compose([\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                      std=[0.229, 0.224, 0.225])\n",
        "                             ])\n",
        "\n",
        "transform_classifier = transforms.Compose([transforms.Resize((224, 224))\n",
        "                                         , transforms.ToTensor()\n",
        "                                         , transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "\n",
        "transfrom_paf = transforms.Compose([transforms.Resize((368, 368))\n",
        "                                         , transforms.ToTensor()\n",
        "                                         , transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])"
      ],
      "metadata": {
        "id": "ZqEAVUH628Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## func"
      ],
      "metadata": {
        "id": "10-Q5vEUSfR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def debugger_is_active() -> bool:\n",
        "    \"\"\"Return if the debugger is currently active\"\"\"\n",
        "    gettrace = getattr(sys, 'gettrace', lambda : None)\n",
        "    return gettrace() is not None\n",
        "\n",
        "def read_dict_from_json(filejson):\n",
        "    if not os.path.isfile(filejson) :\n",
        "        return None\n",
        "    with codecs.open(filejson, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        return obj\n",
        "\n",
        "def make_label_sharpness(args):\n",
        "    pathdir_dest = Path(args.dir_pill_class_base)\n",
        "    dict_pill_prescription_type = read_dict_from_json(args.json_pill_prescription_type)['pill_prescription_type']\n",
        "\n",
        "    list_pills_pres_sharp_score = []\n",
        "    list_pills_otc_sharp_score = []\n",
        "    count = 0\n",
        "    for pathdir_pill_class in pathdir_dest.iterdir():\n",
        "        if not pathdir_pill_class.is_dir() :\n",
        "            continue\n",
        "        print(f'{count}, {pathdir_pill_class} is being estimated')\n",
        "        count += 1\n",
        "        list_pill_class_shape_score = []\n",
        "        for  pathfile_pill_png in pathdir_pill_class.iterdir():\n",
        "            if pathfile_pill_png.suffix == '.json':\n",
        "                continue\n",
        "            score = estimate_sharpness(utils.open_opencv_file(str(pathfile_pill_png)))\n",
        "            list_pill_class_shape_score.append(score)\n",
        "\n",
        "        score_min = min(list_pill_class_shape_score)\n",
        "        score_max = max(list_pill_class_shape_score)\n",
        "        score_mean = sum(list_pill_class_shape_score)/len(list_pill_class_shape_score)\n",
        "        if dict_pill_prescription_type[pathdir_pill_class.stem] == 'PRES':\n",
        "            list_pills_pres_sharp_score.append([str(pathdir_pill_class.stem), score_mean, score_min,score_max])\n",
        "        elif dict_pill_prescription_type[pathdir_pill_class.stem] == 'OTC':\n",
        "            list_pills_otc_sharp_score.append([str(pathdir_pill_class.stem), score_mean, score_min,score_max])\n",
        "\n",
        "    list_pills_pres_sharp_score = sorted(list_pills_pres_sharp_score, key=lambda a: a[2], reverse=True)         # sharp min 기준으로 정렬.\n",
        "    list_pills_otc_sharp_score = sorted(list_pills_otc_sharp_score, key=lambda a: a[2], reverse=True)           # sharp min 기준으로 정렬.\n",
        "    list_pills_label_path_sharp_score = [ [label, pathname, score_mean, score_min, score_max] for label, ( pathname, score_mean, score_min, score_max ) in enumerate(list_pills_pres_sharp_score) if label < 600]\n",
        "    list_pills_label_path_sharp_score += [[label+600, pathname, score_mean, score_min, score_max] for label, (pathname, score_mean, score_min, score_max) in enumerate(list_pills_otc_sharp_score) if label < 400]\n",
        "\n",
        "    dict_temp = { 'pill_label_path_sharp_score':list_pills_label_path_sharp_score}\n",
        "\n",
        "    utils.save_dict_to_json(dict_temp, args.json_pill_label_path_sharp_score)\n",
        "\n",
        "def get_dict_pillid_label(args):\n",
        "    dict_temp = read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_path_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "\n",
        "    dict_label_pillid = {}\n",
        "    for idx, (label, pillid, score_mean, score_min, score_max) in enumerate(list_pills_label_path_sharp_score):\n",
        "        if idx >= args.num_classes:\n",
        "            break\n",
        "        dict_label_pillid.update({pillid:label})\n",
        "\n",
        "    return dict_label_pillid\n",
        "\n",
        "def get_pillid_from_pillfile(file_png):\n",
        "    try :\n",
        "        pill_basename, pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist = Path(file_png).stem.split('_')\n",
        "    except:\n",
        "        # 상위 dir의  name이  pill id가 된다.\n",
        "        path_file = Path(file_png)\n",
        "        pill_basename = path_file.parts[-2]\n",
        "    return pill_basename\n",
        "\n",
        "def get_optimizer(args, model):\n",
        "    optimizer = None\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.wd,\n",
        "            nesterov=args.nesterov\n",
        "        )\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:sgd')\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = optim.Adam( filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:adam')\n",
        "    elif args.optimizer == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.wd,\n",
        "        )\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:rmsprop')\n",
        "    return optimizer\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)      #  maxk, dim=1, largest, sorted -> ( value tensor, index longTensor )\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def model_load(args, model, optimizer, rank=0) :\n",
        "    epoch_begin = 0\n",
        "    state_model = None\n",
        "    state_optimizer = None\n",
        "    if not os.path.isfile(args.model_path_in):\n",
        "        print(f\"-------------------------------------------------->>>>>>>>>>>>> model_path doesn't exist:{args.model_path_in}\")\n",
        "        return  epoch_begin, None, False      # None check point to indicate a fail\n",
        "\n",
        "    if args.verbose == True: print(f\"model_path will be loaded from:{args.model_path_in}\")\n",
        "\n",
        "    dict_checkpoint = torch.load(args.model_path_in, map_location='cpu')\n",
        "\n",
        "    epoch_begin = dict_checkpoint.get('epoch', -1 ) + 1\n",
        "    if rank == 0 :\n",
        "        try:\n",
        "            state_model = dict_checkpoint.get('model', None )\n",
        "            if state_model != None :\n",
        "                if not hasattr(model, 'module') and ('module' in list(state_model.keys())[0] ) :           # model_path은  module 을 포함하고 있다고 가정한다.\n",
        "                    state_model = OrderedDict([ ( k[7:], v ) if 'module' in k else (k,v ) for k,v in state_model.items()])\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'model was loaded from module state')\n",
        "                elif hasattr(model, 'module') and (not 'module' in list(state_model.keys())[0] ) :\n",
        "                    state_model = OrderedDict([('module.' + k, v)  for k, v in state_model.items()])\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'module.model was loaded from normal state')\n",
        "                else:\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'model was loaded from state')\n",
        "            else:\n",
        "                print(f'No model checkpoint in file')\n",
        "\n",
        "        except Exception as e :\n",
        "            print(f'Fail to loading model: {e}')\n",
        "            return epoch_begin, dict_checkpoint, False\n",
        "\n",
        "        if optimizer != None and args.run_phase == 'train':\n",
        "            try:\n",
        "                state_optimizer = dict_checkpoint.get('optimizer', None )\n",
        "                if state_optimizer != None:\n",
        "                    optimizer.load_state_dict(state_optimizer)\n",
        "                    print(f'optimizer was loaded from state')\n",
        "                else:\n",
        "                    print(f'No optimizer checkpoint in file')\n",
        "            except Exception as e :\n",
        "                print(f'Fail to loading optimizer: {e}')\n",
        "                return epoch_begin, dict_checkpoint, False\n",
        "    success = True if state_model != None and state_optimizer != None else False\n",
        "    return epoch_begin, dict_checkpoint, success\n",
        "\n",
        "def pill_classifier(args):\n",
        "    global model, criterion, optimizer, epoch_begin, log_writer\n",
        "    if args.dataset_valid != None:\n",
        "        dataloader_valid = DataLoader(args.dataset_valid, batch_size=args.batch_size, shuffle= False, num_workers=args.num_workers)\n",
        "        print('test')\n",
        "        print(dataloader_valid) ##test\n",
        "    else:\n",
        "        dataloader_valid = None\n",
        "\n",
        "    if args.run_phase == 'train' and args.dataset_train != None:\n",
        "        dataloader_train = DataLoader(args.dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "    else:\n",
        "        dataloader_train = None\n",
        "\n",
        "    if model == None :\n",
        "        log_writer = SummaryWriter(args.dir_log)\n",
        "        if args.cuda == False or torch.cuda.device_count() == 0  :\n",
        "            args.gpu = None\n",
        "        else:\n",
        "            args.gpu = 0\n",
        "\n",
        "        args.rank = args.gpu\n",
        "\n",
        "        cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        model = get_pill_model(args)\n",
        "\n",
        "        # define loss function (criterion) and optimizer\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        if args.cuda:\n",
        "            criterion = criterion.cuda()\n",
        "        optimizer = get_optimizer(args,model)\n",
        "        epoch_begin, dict_checkpoint, success = model_load(args, model, optimizer)\n",
        "\n",
        "    run_model(args, model, dataloader_train, dataloader_valid, None, None,  criterion,optimizer, epoch_begin, log_writer )\n",
        "\n",
        "def run_model(args, model, dataloader_train, dataloader_valid, sampler_train, sampler_valid, criterion,optimizer, epoch_begin, log_writer, verbose=True  ):\n",
        "    if args.run_phase == 'valid' or args.run_phase == 'test':\n",
        "        print(time.asctime())\n",
        "        valid(args, dataloader_valid, sampler_valid,  model, criterion, 0, log_writer)\n",
        "        print(time.asctime())\n",
        "        return\n",
        "\n",
        "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=3, min_lr=0, eps=1e-08)\n",
        "\n",
        "\n",
        "    best_perf = 1000\n",
        "    for epoch in range(epoch_begin, args.epochs):   # ( 0:100)\n",
        "        # adjust_learning_rate(args, optimizer, epoch)\n",
        "        # train for one epoch\n",
        "        perf_indicator = train(args, dataloader_train, sampler_train, model, criterion, optimizer, epoch, log_writer, verbose)\n",
        "        if epoch > 10:\n",
        "            perf_indicator = valid(args, dataloader_valid,sampler_valid, model, criterion, epoch, log_writer,verbose)\n",
        "            if (args.gpu == 0):\n",
        "                print(f'perf_indicator:{perf_indicator} ,  best_perf:{best_perf}')\n",
        "            if perf_indicator < best_perf:\n",
        "                model_save(args.model_path, epoch, model, optimizer, args.rank)\n",
        "                best_perf = perf_indicator\n",
        "        else:\n",
        "            model_save(args.model_path, epoch, model, optimizer, args.rank)\n",
        "            best_perf = perf_indicator\n",
        "\n",
        "        lr_scheduler.step(perf_indicator)\n",
        "\n",
        "def read_dict_from_json(filejson):\n",
        "    if not os.path.isfile(filejson) :\n",
        "        return None\n",
        "    with codecs.open(filejson, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        return obj\n",
        "\n",
        "def get_dict_label_pillid(args):\n",
        "    dict_temp = read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_path_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "\n",
        "    dict_label_pillid = {}\n",
        "    for idx , (label, pillid, score_mean, score_min, score_max) in enumerate(list_pills_label_path_sharp_score) :\n",
        "        if idx >= args.num_classes:\n",
        "            break\n",
        "        # print(f'{label},{pillid},{score_mean},{score_min},{score_max}')\n",
        "        dict_label_pillid.update({label: pillid})\n",
        "\n",
        "    return dict_label_pillid\n",
        "\n",
        "\n",
        "def get_dict_pillid_label(args):\n",
        "    dict_temp = read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_path_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "\n",
        "    dict_label_pillid = {}\n",
        "    for idx, (label, pillid, score_mean, score_min, score_max) in enumerate(list_pills_label_path_sharp_score):\n",
        "        if idx >= args.num_classes:\n",
        "            break\n",
        "        dict_label_pillid.update({pillid:label})\n",
        "\n",
        "    return dict_label_pillid\n",
        "\n",
        "def img_resize(img):\n",
        "  if img.width > img.height:\n",
        "    new_height = img.width\n",
        "    new_image = Image.new(\"RGB\", (img.width, new_height), (1, 1, 1))\n",
        "    y_offset = (new_height - img.height) // 2\n",
        "    new_image.paste(img, (0, y_offset))\n",
        "    return new_image\n",
        "  elif img.height > img.width:\n",
        "    new_width = img.height\n",
        "    new_image = Image.new(\"RGB\", (new_width, img.height), (1, 1, 1))\n",
        "    y_offset = (new_width - img.width) // 2\n",
        "    new_image.paste(img, (0, y_offset))\n",
        "    return new_image\n",
        "  else: # img.height > img.width\n",
        "    return img.resize((244, 244))"
      ],
      "metadata": {
        "id": "68KOo-I1pcPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Gen_Digit"
      ],
      "metadata": {
        "id": "12ycQP1BSkQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gen_Digit():\n",
        "    def __init__(self,args, dir_dataset,run_phase):\n",
        "        print(f'dataset dir is {dir_dataset}')\n",
        "\n",
        "        self.build_list_gen_based_on_level(args)\n",
        "        self.args = self.gen_pill_ready(args, dir_dataset, run_phase)\n",
        "\n",
        "    def build_list_gen_based_on_level(self, args):\n",
        "        global list_aug_geo, list_aug_geo_scale, list_aug_geo_rotate, list_aug_non_geo\n",
        "\n",
        "        print(f'run_phase is {args.run_phase}, aug_level is {args.aug_level}')\n",
        "\n",
        "        for i, (level, aug) in enumerate( list_level_aug_geo):\n",
        "            aug.name = f'g{i}'\n",
        "        for i, (level, aug) in enumerate( list_level_aug_geo_scale):\n",
        "            aug.name = f's{i}'\n",
        "        for i, (level, aug) in enumerate( list_level_aug_geo_rotate):\n",
        "            aug.name = f'r{i}'\n",
        "        for i, (level, aug) in enumerate( list_level_aug_non_geo):\n",
        "            aug.name = f'n{i}'\n",
        "\n",
        "        if args.run_phase == 'train' :\n",
        "            list_aug_geo = [ aug for level, aug in list_level_aug_geo if level <= args.aug_level]\n",
        "            list_aug_geo_scale = [aug for level, aug in list_level_aug_geo_scale if level <= args.aug_level]\n",
        "            list_aug_geo_rotate = [aug for level, aug in list_level_aug_geo_rotate if level <= args.aug_level]\n",
        "            list_aug_non_geo = [aug for level, aug in list_level_aug_non_geo if level <= args.aug_level]\n",
        "        else:\n",
        "            list_aug_geo = [iaa.Identity()]\n",
        "            list_aug_geo_scale = [iaa.Identity()]\n",
        "            list_aug_geo_rotate = [iaa.Identity()]\n",
        "            list_aug_non_geo = [iaa.Identity()]\n",
        "\n",
        "    def gen_pill_ready(self, args, dir_dataset, run_phase):\n",
        "        global list_aug_geo, list_aug_geo_scale, list_aug_geo_rotate, list_aug_non_geo\n",
        "        print(f'gen_type is {args.gen_type}, loading data ...')\n",
        "\n",
        "        path_dir_json = Path(dir_dataset)\n",
        "\n",
        "        if path_dir_json.is_dir() :\n",
        "            print(f'Gen :reading directory was not implemented')\n",
        "            self.len_total = 0\n",
        "\n",
        "        elif path_dir_json.is_file() and path_dir_json.suffix == '.json' and args.gen_type == 'read_only_image':\n",
        "\n",
        "            dict_temp = read_dict_from_json(str(path_dir_json))\n",
        "            self.list_label_path = []\n",
        "            if args.gen_dataclass_sel in [ 'dataclass0', 'dataclass01'] :\n",
        "                if run_phase == 'train' :\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class0_train', [])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class0_train >>>')\n",
        "                elif run_phase == 'valid' :\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class0_valid', [])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class0_valid >>>')\n",
        "                else:\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class0_test', [])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class0_test >>>')\n",
        "\n",
        "            if args.gen_dataclass_sel in [ 'dataclass1', 'dataclass01'] :\n",
        "                if run_phase == 'train' :\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class1_train', [])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class1_train >>>')\n",
        "                elif run_phase == 'valid' :\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class1_valid', [])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class1_valid >>>')\n",
        "                else:\n",
        "                    self.list_label_path += dict_temp.get('pngfile_class1_test',[])\n",
        "                    print(f'label_path was loaded from   <<< pngfile_class1_test >>>')\n",
        "\n",
        "            dict_pillid_label = get_dict_pillid_label(args)\n",
        "            self.list_label_path = [(dict_pillid_label[get_pillid_from_pillfile(pngfile)], pngfile) for pngfile in self.list_label_path]\n",
        "\n",
        "            self.list_aug_geo = list_aug_geo\n",
        "            self.list_aug_geo_scale = list_aug_geo_scale\n",
        "            self.list_aug_geo_rotate = list_aug_geo_rotate\n",
        "            self.list_aug_non_geo = list_aug_non_geo\n",
        "\n",
        "            self.len_list_aug_geo = len(self.list_aug_geo)\n",
        "            self.len_list_aug_geo_scale = len(self.list_aug_geo_scale)\n",
        "            self.len_list_aug_geo_rotate = len(self.list_aug_geo_rotate)\n",
        "            self.len_list_aug_non_geo = len(self.list_aug_non_geo)\n",
        "            self.len_list_label_path = len(self.list_label_path)\n",
        "\n",
        "            self.len_total = self.len_list_aug_geo * self.len_list_aug_geo_scale * self.len_list_aug_geo_rotate * self.len_list_aug_non_geo * self.len_list_label_path\n",
        "            self.div_aug_geo = self.len_list_aug_geo_scale * self.len_list_aug_geo_rotate * self.len_list_aug_non_geo * self.len_list_label_path\n",
        "            self.div_aug_geo_scale = self.len_list_aug_geo_rotate * self.len_list_aug_non_geo * self.len_list_label_path\n",
        "            self.div_aug_geo_rotate = self.len_list_aug_non_geo * self.len_list_label_path\n",
        "            self.div_aug_non_geo = self.len_list_label_path\n",
        "\n",
        "        print(f\"data loading done.  dataset'length is {self.len_total}\")\n",
        "        return args\n",
        "\n",
        "\n",
        "    def generate_digits_by_index(self,args, index ) :\n",
        "        if args.gen_type == 'read_only_image' :\n",
        "\n",
        "            ind = index // self.div_aug_geo\n",
        "            mod = index % self.div_aug_geo\n",
        "\n",
        "            aug_geo = self.list_aug_geo[ind]\n",
        "\n",
        "            ind = mod // self.div_aug_geo_scale\n",
        "            mod = mod % self.div_aug_geo_scale\n",
        "\n",
        "            aug_geo_scale = self.list_aug_geo_scale[ind]\n",
        "\n",
        "            ind = mod // self.div_aug_geo_rotate\n",
        "            mod = mod % self.div_aug_geo_rotate\n",
        "\n",
        "            aug_geo_rotate = self.list_aug_geo_rotate[ind]\n",
        "\n",
        "            ind = mod // self.div_aug_non_geo\n",
        "            mod = mod % self.div_aug_non_geo\n",
        "\n",
        "            aug_non_geo = self.list_aug_non_geo[ind]\n",
        "            label, path_img = self.list_label_path[mod]\n",
        "            np_image = np.array(Image.open(path_img))\n",
        "\n",
        "            # np_image= aug_geo(image=np_image)\n",
        "            # np_image= aug_geo_scale(image=np_image)\n",
        "            # np_image= aug_geo_rotate(image=np_image)\n",
        "            # np_image= aug_non_geo(image=np_image)\n",
        "\n",
        "            iaaaug = iaa.Sequential([aug_geo, aug_geo_scale, aug_geo_rotate, aug_non_geo])\n",
        "            np_image = iaaaug(image=np_image)\n",
        "\n",
        "            aug_name = aug_geo.name + aug_geo_scale.name + aug_geo_rotate.name + aug_non_geo.name\n",
        "\n",
        "            return np_image, label, path_img, aug_name"
      ],
      "metadata": {
        "id": "DOoHwlkVP1DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Dataset_Pill"
      ],
      "metadata": {
        "id": "zHHUqQdYSomI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Pill(Dataset):\n",
        "    def __init__(self, args, dir_dataset, transform=None, target_transform=None, run_phase='train'):\n",
        "        self.args = args\n",
        "        self.gen_digit = Gen_Digit(args, dir_dataset, run_phase)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.run_phase = run_phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.gen_digit.len_total\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label, path_img, aug_name = self.gen_digit.generate_digits_by_index(self.args, idx)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "        if self.run_phase == 'valid' or self.run_phase == 'test':\n",
        "            return image, label, path_img, aug_name\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "def get_pill_model(args):\n",
        "\n",
        "    if args.cnn_name == 'resnet152' :\n",
        "        model = models.resnet152(num_classes=args.num_classes)\n",
        "    elif args.cnn_name == 'hrnet_w64' :\n",
        "        model = get_hrnet()\n",
        "        model.classifier = nn.Linear(in_features=2048, out_features=args.num_classes, bias=True)\n",
        "    else:\n",
        "        raise Exception('No Found CNN Name')\n",
        "\n",
        "    if args.cuda == True:\n",
        "        if args.gpu is not None:\n",
        "            model.cuda(args.gpu)\n",
        "        else:\n",
        "            model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train(args, dataloader, sampler, model, criterion, optimizer, epoch, log_writer=None, verbose=True):\n",
        "    model.train()\n",
        "    if sampler != None:\n",
        "        sampler.set_epoch(epoch)\n",
        "\n",
        "    metric_train_loss = AverageMeter()\n",
        "    ametric_data_time = AverageMeter()\n",
        "\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    with tqdm(total=len(dataloader), desc=args.tqdm_desc_head + 'Train Epoch  #{}'.format(epoch), disable=not verbose) as t:\n",
        "        for batch_idx, (img, target) in enumerate(dataloader):\n",
        "            if args.cuda:\n",
        "                img = img.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(img)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            prec1, prec5 = accuracy(output, target, (1, 5))\n",
        "            count_try = img.cpu().shape[0]\n",
        "            top1.update(prec1[0].detach().cpu().item(), count_try)\n",
        "            top5.update(prec5[0].detach().cpu().item(), count_try)\n",
        "\n",
        "            metric_train_loss.update(loss.detach().cpu().item(), count_try)\n",
        "            t.set_postfix({'loss': metric_train_loss.avg, 'lr':lr,  'top1':top1.avg, 'top5':top5.avg })\n",
        "            t.update(1)\n",
        "\n",
        "    ametric_data_time.update(time.time() - end)\n",
        "\n",
        "    if log_writer:\n",
        "        log_writer.add_scalar('train/loss', metric_train_loss.avg, epoch)\n",
        "\n",
        "    try:\n",
        "        print_string = 'Epoch: [{0}][{1}/{2}]\\t'.format(epoch, batch_idx, len(dataloader))\n",
        "        print_string += 'Data time {data_time.val:.3f} ({data_time.avg:.3f} Now:{Now})\\t'.format(data_time=ametric_data_time, Now=datetime.datetime.now())\n",
        "        print_string += 'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=metric_train_loss)\n",
        "        print_string += 'Accuracy top1:{top1.avg:.4f}, top5:{top5.avg:.4f}'.format(top1=top1, top5=top5)\n",
        "        print(print_string)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return metric_train_loss.avg\n",
        "\n",
        "\n",
        "def valid(args, dataloader, sampler,  model, criterion, epoch, log_writer=None, verbose=True):\n",
        "    metric_train_loss = AverageMeter()\n",
        "    ametric_data_time = AverageMeter()\n",
        "\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    if sampler != None:\n",
        "        sampler.set_epoch(epoch)\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    args.list_preds = []\n",
        "    args.list_target = []\n",
        "    args.count_correct = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(dataloader), desc=args.tqdm_desc_head + '{} Epoch  #{}'.format( args.run_phase, epoch), disable=not verbose) as t:\n",
        "            for i, (img, target, path_img, aug_name ) in enumerate(dataloader):\n",
        "                # measure data loading time\n",
        "\n",
        "                if args.cuda:\n",
        "                    img = img.cuda()\n",
        "                    target = target.cuda()\n",
        "\n",
        "                output = model(img)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                prec1, prec5 = accuracy(output, target, (1, 5))\n",
        "\n",
        "                if ( prec1[0].detach().cpu().item() != 100.):\n",
        "                    if args.run_phase == 'valid': print(f'<------- class valid fail file: {path_img[0]}, aug_name:{aug_name}')\n",
        "\n",
        "                preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "                count_correct = preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "                list_preds = preds.view(-1).tolist()\n",
        "\n",
        "                args.list_preds = list_preds\n",
        "                args.list_target = target.detach().cpu().tolist()\n",
        "                args.count_correct = count_correct.item()\n",
        "                count_try = img.cpu().shape[0]\n",
        "                top1.update(prec1[0].detach().cpu().item(), count_try)\n",
        "                top5.update(prec5[0].detach().cpu().item(), count_try)\n",
        "\n",
        "                metric_train_loss.update(loss.detach().cpu().item(), count_try)\n",
        "\n",
        "                t.set_postfix({'loss': metric_train_loss.avg, 'top1': top1.avg, 'top5': top5.avg})\n",
        "                t.update(1)\n",
        "\n",
        "    ametric_data_time.update(time.time() - end)\n",
        "\n",
        "    if log_writer:\n",
        "        log_writer.add_scalar('validation/loss', metric_train_loss.avg, epoch)\n",
        "\n",
        "    try:\n",
        "        print_string = 'Epoch: [{0}][{1}/{2}]\\t'.format(epoch, i, len(dataloader))\n",
        "        print_string += 'Data time {data_time.val:.3f} ({data_time.avg:.3f} Now:{Now})\\t'.format(data_time=ametric_data_time, Now=datetime.datetime.now())\n",
        "        print_string += 'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=metric_train_loss)\n",
        "        print_string += 'Accuracy top1:{top1.avg:.4f}, top5:{top5.avg:.4f}'.format(top1=top1, top5=top5)\n",
        "        print(print_string)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return metric_train_loss.avg\n",
        "# if __name__ == '__main__':\n",
        "#     # job = 'hrnet_w64'\n",
        "#     job = 'resnet152'\n",
        "#     args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='0')\n",
        "\n",
        "#     print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "#     end = time.time()\n",
        "#     if args.run_phase == 'train'  :\n",
        "#         args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "#         print(f'train dataset was loaded')\n",
        "\n",
        "#     args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "#     print(f'valid dataset was loaded')\n",
        "\n",
        "\n",
        "#     print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "#     pill_classifier(args)\n",
        "#     print('job done')"
      ],
      "metadata": {
        "id": "JGPf21KJI8dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class AverageMeter"
      ],
      "metadata": {
        "id": "jkX4Ty4Nl1Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"  val을 계속 누적하고,  평균을 구함.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.sum / self.count\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self.sum"
      ],
      "metadata": {
        "id": "HDT8JC7Wl0BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Args"
      ],
      "metadata": {
        "id": "z35f8n_GStH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_args(job='resnet152', run_phase = 'train', aug_level=0, dataclass='0'):\n",
        "    #######################################################################################################\n",
        "    print(f'job={job} run_phase:{run_phase} aug_level:{aug_level}, dataclass:{dataclass} ')\n",
        "    verbose = True\n",
        "\n",
        "    dir_solution_home = r'/content/drive/MyDrive/data/'\n",
        "    num_workers = 4\n",
        "    num_threads = 2\n",
        "    BATCH_SIZE = 8\n",
        "    dist_backend = 'nccl'\n",
        "\n",
        "    if run_phase == 'valid'  and run_phase == 'test':\n",
        "        BATCH_SIZE = 1\n",
        "        num_workers = 1\n",
        "        num_threads = 1\n",
        "        verbose = False\n",
        "\n",
        "    DIR_DATA = os.path.join(dir_solution_home, 'pill_data')\n",
        "    DIR_PROJ = os.path.join(dir_solution_home, 'proj_pill')\n",
        "\n",
        "\n",
        "    # define directory\n",
        "    dir_pill_class_base = 'pill_data_croped'\n",
        "    json_pill_label_path_sharp_score = 'pill_label_path_sharp_score.json'\n",
        "    json_pill_prescription_type = 'pill_prescription_type.json'\n",
        "    json_pill_class_list = 'pill_class_list.json'\n",
        "\n",
        "    FITTOSIZE = 224\n",
        "    gen_type = 'read_only_image'\n",
        "    dataclass = f'dataclass{dataclass}'\n",
        "\n",
        "    if job == 'resnet152':\n",
        "        model_path_in = os.path.join(DIR_PROJ, f'pill_resnet152_{dataclass}_aug{aug_level}.pt')\n",
        "        model_path = os.path.join(DIR_PROJ, f'pill_resnet152_{dataclass}_aug{aug_level}.pt')\n",
        "\n",
        "    elif job == 'hrnet_w64':\n",
        "        model_path_in = os.path.join(DIR_PROJ, f'pill_hrnet_w64_{dataclass}_aug{aug_level}.pt')\n",
        "        model_path = os.path.join(DIR_PROJ, f'pill_hrnet_w64_{dataclass}_aug{aug_level}.pt')\n",
        "\n",
        "\n",
        "    #######################################################################################################\n",
        "\n",
        "\n",
        "    dir_pill_class_base = os.path.join(DIR_DATA, dir_pill_class_base)\n",
        "    json_pill_label_path_sharp_score = os.path.join(dir_pill_class_base, json_pill_label_path_sharp_score)\n",
        "    json_pill_prescription_type = os.path.join(dir_pill_class_base, json_pill_prescription_type)\n",
        "    dir_output = os.path.join(DIR_DATA, 'output')  # output dir for generation from gauge info\n",
        "\n",
        "\n",
        "\n",
        "    dir_log = './logs'\n",
        "    file_log = os.path.join(dir_log, f'log-{job}.txt')\n",
        "\n",
        "    #######################################################################################################\n",
        "    tqdm_desc_head = f'{job} aug_level:{aug_level} :'\n",
        "    print(f'BATCH_SIZE:{BATCH_SIZE}, num_workers:{num_workers}, num_threads:{num_threads} ')\n",
        "    #######################################################################################################\n",
        "    args = SimpleNamespace()\n",
        "\n",
        "    args.size_image = FITTOSIZE\n",
        "    args.dir_log = dir_log\n",
        "    args.dir_pill_class_base = dir_pill_class_base\n",
        "    args.json_pill_label_path_sharp_score = json_pill_label_path_sharp_score\n",
        "    args.json_pill_prescription_type = json_pill_prescription_type\n",
        "    args.pill_dataset_class0 = [90, 75]\n",
        "    args.pill_dataset_class1 = [70, 60]\n",
        "    args.pill_dataset_train_rate = 0.8\n",
        "    args.pill_dataset_valid_rate = 0.1\n",
        "    args.pill_dataset_test_rate = 0.1\n",
        "    args.num_classes = 1000\n",
        "\n",
        "    json_pill_class_list  = os.path.join(dir_pill_class_base, json_pill_class_list )\n",
        "    args.json_pill_class_list = json_pill_class_list\n",
        "    args.gen_type = gen_type\n",
        "    args.gen_dataclass_sel = dataclass\n",
        "    args.cnn_name = job\n",
        "    args.batches_per_allreduce = 1\n",
        "    args.fp16_allreduce = False\n",
        "    args.use_adasum = False\n",
        "    args.gradient_predivide_factor = 1.0\n",
        "    args.wd = 0.0001\n",
        "    args.momentum = 0.9\n",
        "    args.warmup_epochs = 5\n",
        "    args.epochs = 150\n",
        "    args.disable_cuda = False\n",
        "    args.base_lr = 0.001\n",
        "    args.lr = 0.001\n",
        "    args.pre_lr = 1e-4\n",
        "    args.lr_schedule=[60, 100, 140]\n",
        "    args.lr_gamma = 0.1\n",
        "    args.lr_factor = 0.1\n",
        "    args.batch_size = BATCH_SIZE\n",
        "    args.allreduce_batch_size = 8\n",
        "    args.run_phase = run_phase\n",
        "    args.optimizer ='sgd'\n",
        "    args.model_path = model_path\n",
        "    args.model_path_in = model_path_in\n",
        "    vgg19_path = os.path.join(DIR_PROJ, 'paf_vgg19_level{aug_level}.pt')\n",
        "\n",
        "    dir_gauge_json = os.path.join(dir_solution_home, '세온', 'digitGaugeSamples')\n",
        "    dir_gauge_json = os.path.join(dir_solution_home, '세온', 'digitGaugeSamples2')\n",
        "    dir_gauge_json = os.path.join(dir_solution_home, '세온', 'digitGaugeSamples3')\n",
        "    dir_gauge_json = os.path.join(dir_solution_home, '세온', 'digitGaugeTest')\n",
        "    dir_class_basic = os.path.join(DIR_DATA, 'digit_class_base')\n",
        "    dir_class_basic_aug = os.path.join(DIR_DATA, 'digit_class_base_aug')\n",
        "\n",
        "    dir_digit_back = os.path.join(DIR_DATA, 'backimage')  # back image\n",
        "    dir_digit_anno = os.path.join(DIR_DATA, 'annotation')  # annotation\n",
        "    dir_digit_annoimage = os.path.join(DIR_DATA, 'annotationImage')  # annotation + image\n",
        "\n",
        "    dir_digit_paf = os.path.join(DIR_DATA, 'digit_paf_train')  # annotation + image\n",
        "    dir_digit_heat = os.path.join(DIR_DATA, 'digit_heat_train')  # annotation + image\n",
        "\n",
        "    args.pre_n_images = 8000\n",
        "    args.n_images = None\n",
        "    args.duplicate_data=None\n",
        "    args.nesterov = True\n",
        "    args.print_freq = 10\n",
        "    args.freeze_base=0\n",
        "    args.update_batchnorm_runningstatistics = False\n",
        "    args.stride = 8\n",
        "    args.train_scale = 5\n",
        "    args.num_pts_between_keypoints = 10\n",
        "    args.ema = 1e-3\n",
        "    args.debug_without_plots = False\n",
        "    args.category_name = 'digit0'\n",
        "    args.model_factor = 8\n",
        "    args.heatmap_threshold = 0.05\n",
        "    args.pafmap_threshold = 0.01\n",
        "    args.dataset_type = 'GenDigit'\n",
        "    args.output = 'output.json'\n",
        "    args.dir_output = dir_output\n",
        "    args.aug_level = aug_level\n",
        "    args.dir_digit_anno = dir_digit_anno\n",
        "    args.dir_digit_back = dir_digit_back\n",
        "    args.dir_digit_annoimage = dir_digit_annoimage\n",
        "    args.gen_digit_img_load_type = 'partial_load'\n",
        "    args.fittosize = FITTOSIZE\n",
        "    args.normal = 0.7\n",
        "    pickle_list_cv_heat_paf_path_augname_label_level1 = os.path.join(DIR_DATA, 'data', 'list_cv_heat_paf_path_augname_label_level1.pickle')\n",
        "    pickle_list_cv_heat_paf_path_augname_label_level2 = os.path.join(DIR_DATA, 'data', 'list_cv_heat_paf_path_augname_label_level2.pickle')\n",
        "    args.world_size = num_threads\n",
        "    args.rank = 1\n",
        "    args.dist_url = 'tcp://127.0.0.1:23456'\n",
        "    args.dist_backend = dist_backend\n",
        "    args.seed = 41\n",
        "    args.gpu = None\n",
        "    args.multiprocessing_distributed = True\n",
        "    args.pickle_list_cv_heat_paf_path_augname_label_level1 = pickle_list_cv_heat_paf_path_augname_label_level1\n",
        "    args.pickle_list_cv_heat_paf_path_augname_label_level2 = pickle_list_cv_heat_paf_path_augname_label_level2\n",
        "    args.verbose = verbose\n",
        "    args.tqdm_desc_head = tqdm_desc_head\n",
        "    args.num_workers = num_workers\n",
        "    args.num_threads = num_threads\n",
        "    args.dir_gauge_json = dir_gauge_json\n",
        "    args.dir_class_basic = dir_class_basic\n",
        "    args.train_ratio = 0.8\n",
        "    args.dir_class_basic_aug = dir_class_basic_aug\n",
        "    args.dir_digit_paf = dir_digit_paf\n",
        "    args.dir_digit_heat = dir_digit_heat\n",
        "    args.device = torch.device('cpu')\n",
        "    args.pin_memory = False\n",
        "    args.cuda = False\n",
        "    if not args.disable_cuda and torch.cuda.is_available():\n",
        "        args.device = torch.device('cuda')\n",
        "        args.pin_memory = True\n",
        "        args.cuda = True\n",
        "\n",
        "    # os.makedirs(args.dir_output, exist_ok=True)\n",
        "    os.makedirs(args.dir_log, exist_ok=True)\n",
        "\n",
        "    return args\n",
        "\n"
      ],
      "metadata": {
        "id": "bIJx6lam8_MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_pic(pic):\n",
        "  size=(244,244)\n",
        "  base_pic=np.zeros((size[1],size[0],3),np.uint8)\n",
        "  pic1=cv2.imread(pic,cv2.IMREAD_COLOR)\n",
        "  h,w=pic1.shape[:2]\n",
        "  ash=size[1]/h\n",
        "  asw=size[0]/w\n",
        "  if asw<ash:\n",
        "      sizeas=(int(w*asw),int(h*asw))\n",
        "  else:\n",
        "      sizeas=(int(w*ash),int(h*ash))\n",
        "  pic1 = cv2.resize(pic1,dsize=sizeas)\n",
        "  base_pic[int(size[1]/2-sizeas[1]/2):int(size[1]/2+sizeas[1]/2),\n",
        "  int(size[0]/2-sizeas[0]/2):int(size[0]/2+sizeas[0]/2),:]=pic1\n",
        "  cv2.imwrite(new_fol+'/'+pic,base_pic)"
      ],
      "metadata": {
        "id": "pJBtXbMd8ynP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model load"
      ],
      "metadata": {
        "id": "vwO2fbIilhWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_args(job=job, run_phase='test', aug_level=0, dataclass='01')\n",
        "resnet = models.resnet152()\n",
        "dict_checkpoint = torch.load(args.model_path_in, map_location='cpu')\n",
        "\n",
        "epoch_begin = 0\n",
        "state_model = None\n",
        "state_optimizer = None\n",
        "epoch_begin = dict_checkpoint.get('epoch', -1 ) + 1\n",
        "state_model = dict_checkpoint.get('model', None )\n",
        "\n",
        "if not hasattr(resnet, 'module') and ('module' in list(state_model.keys())[0] ) :           # model_path은  module 을 포함하고 있다고 가정한다.\n",
        "    state_model = OrderedDict([ ( k[7:], v ) if 'module' in k else (k,v ) for k,v in state_model.items()])\n",
        "    resnet.load_state_dict(state_model)\n",
        "    print(f'model was loaded from module state')\n",
        "elif hasattr(resnet, 'module') and (not 'module' in list(state_model.keys())[0] ) :\n",
        "    state_model = OrderedDict([('module.' + k, v)  for k, v in state_model.items()])\n",
        "    resnet.load_state_dict(state_model)\n",
        "    print(f'module.model was loaded from normal state')\n",
        "else:\n",
        "    resnet.load_state_dict(state_model)\n",
        "    print(f'model was loaded from state')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz36USCuOYn6",
        "outputId": "f71a5c65-3a1d-4acf-f931-78254848e06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job=resnet152 run_phase:test aug_level:0, dataclass:01 \n",
            "BATCH_SIZE:8, num_workers:4, num_threads:2 \n",
            "model was loaded from state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **실행파일**"
      ],
      "metadata": {
        "id": "5EZh_upBQtaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_list = os.listdir(\"/content/drive/MyDrive/Yolov8_Model/data/val\")\n",
        "val_list = [file for file in val_list if file.endswith('.png')]\n",
        "\n",
        "for n in val_list:\n",
        "  imgsPath = \"/content/drive/MyDrive/Yolov8_Model/data/val\"\n",
        "  imgName = n\n",
        "  img_Full_Path = imgsPath+\"/\"+imgName\n",
        "  yolo_img_labels = [\"K\"+imgName[1:8],\"K\"+imgName[8:15],\"K\"+imgName[15:22],\"K\"+imgName[22:29]]\n",
        "\n",
        "  result = Yolo.predict(img_Full_Path)\n",
        "\n",
        "  labels = result[0].boxes.data.tolist()\n",
        "  labelList = []\n",
        "  for i in labels:\n",
        "    labelList += [i[0:4]]\n",
        "\n",
        "  for idx, l in enumerate(labelList):\n",
        "    x1,y1,x2,y2 = l\n",
        "    yolo_label = yolo_img_labels[idx]\n",
        "    img = Image.open(img_Full_Path)\n",
        "\n",
        "    crop_img = img.crop((x1,y1,x2,y2))\n",
        "    img.close()\n",
        "    resnet_input = img_resize(crop_img)\n",
        "    resnet_input = resnet_input.resize((244,244))\n",
        "    print(resnet_input)\n",
        "\n",
        "    # resnet_input = resnet_input.resize((244,244))\n",
        "    resnet.eval()\n",
        "    ti = transform_normalize(resnet_input)\n",
        "    ti = ti.unsqueeze(0)\n",
        "    outputs = resnet(ti)\n",
        "\n",
        "    print(imgName)\n",
        "    print(get_dict_label_pillid(args)[outputs.data.max(dim=1, keepdim=True)[1][0][0].item()])\n",
        "    # try:\n",
        "    #   print(yolo_img_labels[idx])\n",
        "    #   print(outputs.data.max(dim=1, keepdim=True)[1] == get_dict_pillid_label(args)[yolo_label])\n",
        "    # except:\n",
        "    #   None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hyLQ5vWSz8Lr",
        "outputId": "19650dfb-3e57-4087-8a05-39fab082f269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-010224-016548-031705_0_2_0_2_90_000_200.png: 640x512 4 pills, 1114.2ms\n",
            "Speed: 2.1ms preprocess, 1114.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6106A0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_90_000_200.png\n",
            "K-037446\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610CA0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_90_000_200.png\n",
            "K-019196\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6130D0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_90_000_200.png\n",
            "K-001061\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613970>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-010224-016548-031705_0_2_0_2_90_000_200.png\n",
            "K-038187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-010224-016548-031705_0_2_0_2_70_000_200.png: 640x512 4 pills, 1119.3ms\n",
            "Speed: 1.8ms preprocess, 1119.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610FD0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_70_000_200.png\n",
            "K-049428\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613AF0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_70_000_200.png\n",
            "K-005466\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610FD0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_70_000_200.png\n",
            "K-023835\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613BB0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-010224-016548-031705_0_2_0_2_70_000_200.png\n",
            "K-018110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-010224-016548-031705_0_2_0_2_75_000_200.png: 640x512 4 pills, 830.6ms\n",
            "Speed: 1.9ms preprocess, 830.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610160>\n",
            "K-001900-010224-016548-031705_0_2_0_2_75_000_200.png\n",
            "K-017220\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613BB0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_75_000_200.png\n",
            "K-019196\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6137F0>\n",
            "K-001900-010224-016548-031705_0_2_0_2_75_000_200.png\n",
            "K-001061\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612320>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-010224-016548-031705_0_2_0_2_75_000_200.png\n",
            "K-045862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029451_0_2_0_2_90_000_200.png: 640x512 4 pills, 828.2ms\n",
            "Speed: 1.8ms preprocess, 828.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6105B0>\n",
            "K-001900-003544-016548-029451_0_2_0_2_90_000_200.png\n",
            "K-000317\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612320>\n",
            "K-001900-003544-016548-029451_0_2_0_2_90_000_200.png\n",
            "K-011166\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6105B0>\n",
            "K-001900-003544-016548-029451_0_2_0_2_90_000_200.png\n",
            "K-036172\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613970>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029451_0_2_0_2_90_000_200.png\n",
            "K-018188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029451_0_2_0_2_75_000_200.png: 640x512 4 pills, 830.0ms\n",
            "Speed: 1.9ms preprocess, 830.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6109D0>\n",
            "K-001900-003544-016548-029451_0_2_0_2_75_000_200.png\n",
            "K-000317\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612BF0>\n",
            "K-001900-003544-016548-029451_0_2_0_2_75_000_200.png\n",
            "K-036172\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612440>\n",
            "K-001900-003544-016548-029451_0_2_0_2_75_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A611810>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029451_0_2_0_2_75_000_200.png\n",
            "K-033746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029451_0_2_0_2_70_000_200.png: 640x512 4 pills, 842.9ms\n",
            "Speed: 3.1ms preprocess, 842.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610460>\n",
            "K-001900-003544-016548-029451_0_2_0_2_70_000_200.png\n",
            "K-000317\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A611810>\n",
            "K-001900-003544-016548-029451_0_2_0_2_70_000_200.png\n",
            "K-033746\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610460>\n",
            "K-001900-003544-016548-029451_0_2_0_2_70_000_200.png\n",
            "K-007340\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6101C0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029451_0_2_0_2_70_000_200.png\n",
            "K-036172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029451_0_2_0_2_75_000_200.png: 640x512 4 pills, 1052.6ms\n",
            "Speed: 2.0ms preprocess, 1052.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612440>\n",
            "K-001900-003544-016551-029451_0_2_0_2_75_000_200.png\n",
            "K-007144\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610DF0>\n",
            "K-001900-003544-016551-029451_0_2_0_2_75_000_200.png\n",
            "K-036172\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD196BFA980>\n",
            "K-001900-003544-016551-029451_0_2_0_2_75_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610DF0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029451_0_2_0_2_75_000_200.png\n",
            "K-030239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029451_0_2_0_2_90_000_200.png: 640x512 4 pills, 834.2ms\n",
            "Speed: 1.6ms preprocess, 834.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613970>\n",
            "K-001900-003544-016551-029451_0_2_0_2_90_000_200.png\n",
            "K-007144\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612320>\n",
            "K-001900-003544-016551-029451_0_2_0_2_90_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6130D0>\n",
            "K-001900-003544-016551-029451_0_2_0_2_90_000_200.png\n",
            "K-036172\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613250>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029451_0_2_0_2_90_000_200.png\n",
            "K-018188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029451_0_2_0_2_70_000_200.png: 640x512 4 pills, 820.5ms\n",
            "Speed: 2.2ms preprocess, 820.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610CA0>\n",
            "K-001900-003544-016551-029451_0_2_0_2_70_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613250>\n",
            "K-001900-003544-016551-029451_0_2_0_2_70_000_200.png\n",
            "K-011154\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6137C0>\n",
            "K-001900-003544-016551-029451_0_2_0_2_70_000_200.png\n",
            "K-007340\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6107F0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029451_0_2_0_2_70_000_200.png\n",
            "K-036172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029345_0_2_0_2_70_000_200.png: 640x512 4 pills, 804.8ms\n",
            "Speed: 1.6ms preprocess, 804.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612320>\n",
            "K-001900-003544-016548-029345_0_2_0_2_70_000_200.png\n",
            "K-019860\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610940>\n",
            "K-001900-003544-016548-029345_0_2_0_2_70_000_200.png\n",
            "K-005466\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613D30>\n",
            "K-001900-003544-016548-029345_0_2_0_2_70_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610940>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029345_0_2_0_2_70_000_200.png\n",
            "K-014084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029345_0_2_0_2_75_000_200.png: 640x512 4 pills, 841.3ms\n",
            "Speed: 1.7ms preprocess, 841.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613760>\n",
            "K-001900-003544-016548-029345_0_2_0_2_75_000_200.png\n",
            "K-052276\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6105B0>\n",
            "K-001900-003544-016548-029345_0_2_0_2_75_000_200.png\n",
            "K-037446\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610460>\n",
            "K-001900-003544-016548-029345_0_2_0_2_75_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612800>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029345_0_2_0_2_75_000_200.png\n",
            "K-007144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016548-029345_0_2_0_2_90_000_200.png: 640x512 4 pills, 958.6ms\n",
            "Speed: 1.9ms preprocess, 958.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613AF0>\n",
            "K-001900-003544-016548-029345_0_2_0_2_90_000_200.png\n",
            "K-000317\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6119F0>\n",
            "K-001900-003544-016548-029345_0_2_0_2_90_000_200.png\n",
            "K-029534\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613D60>\n",
            "K-001900-003544-016548-029345_0_2_0_2_90_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6102E0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016548-029345_0_2_0_2_90_000_200.png\n",
            "K-028360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029345_0_2_0_2_70_000_200.png: 640x512 4 pills, 823.0ms\n",
            "Speed: 1.7ms preprocess, 823.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6109D0>\n",
            "K-001900-003544-016551-029345_0_2_0_2_70_000_200.png\n",
            "K-012758\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613BB0>\n",
            "K-001900-003544-016551-029345_0_2_0_2_70_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613760>\n",
            "K-001900-003544-016551-029345_0_2_0_2_70_000_200.png\n",
            "K-005466\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613850>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029345_0_2_0_2_70_000_200.png\n",
            "K-010224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029345_0_2_0_2_90_000_200.png: 640x512 4 pills, 850.7ms\n",
            "Speed: 2.0ms preprocess, 850.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610940>\n",
            "K-001900-003544-016551-029345_0_2_0_2_90_000_200.png\n",
            "K-029534\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613850>\n",
            "K-001900-003544-016551-029345_0_2_0_2_90_000_200.png\n",
            "K-029534\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A610940>\n",
            "K-001900-003544-016551-029345_0_2_0_2_90_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A612BF0>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029345_0_2_0_2_90_000_200.png\n",
            "K-028360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "image 1/1 /content/drive/MyDrive/Yolov8_Model/data/val/K-001900-003544-016551-029345_0_2_0_2_75_000_200.png: 640x512 4 pills, 826.0ms\n",
            "Speed: 1.9ms preprocess, 826.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613760>\n",
            "K-001900-003544-016551-029345_0_2_0_2_75_000_200.png\n",
            "K-019196\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A6109D0>\n",
            "K-001900-003544-016551-029345_0_2_0_2_75_000_200.png\n",
            "K-037446\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613760>\n",
            "K-001900-003544-016551-029345_0_2_0_2_75_000_200.png\n",
            "K-000114\n",
            "<PIL.Image.Image image mode=RGB size=244x244 at 0x7FD17A613250>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-001900-003544-016551-029345_0_2_0_2_75_000_200.png\n",
            "K-007144\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-437140d96933>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0myolo_img_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mimgName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_Full_Path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_prompts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# Postprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         visualize = increment_path(self.save_dir / Path(self.batch[0][0]).stem,\n\u001b[1;32m    132\u001b[0m                                    mkdir=True) if self.args.visualize and (not self.source_type.tensor) else False\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/autobackend.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_module\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TorchScript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m_predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_one_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/block.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;34m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/block.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;34m\"\"\"'forward()' applies the YOLOv5 FPN to input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_fuse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_input = Image.open('/content/drive/MyDrive/data/pill_data/pill_data_croped/K-000059/K-000059_0_0_0_0_60_180_200.png')\n",
        "ti = transform_normalize(resnet_input)\n",
        "ti = ti.unsqueeze(0)\n",
        "outputs = resnet(ti)\n",
        "print(get_dict_label_pillid(args)[outputs.data.max(dim=1, keepdim=True)[1][0][0].item()])\n",
        "# try:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FrFdwXi8-Bu",
        "outputId": "33bf93df-f936-42b9-ef99-6e150229aea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-000059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Image.open('/content/drive/MyDrive/data/pill_data/pill_data_croped/K-000059/K-000059_0_0_0_0_60_180_200.png'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW5kBz-tbIbn",
        "outputId": "2c481a80-20ca-4022-e284-e511c29b1011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7FD1998EE470>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = get_args(job=job, run_phase='test', aug_level=0, dataclass='0')\n",
        "\n",
        "print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "end = time.time()\n",
        "if args.run_phase == 'train'  :\n",
        "    args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "    print(f'train dataset was loaded')\n",
        "\n",
        "args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "print(f'valid dataset was loaded')\n",
        "\n",
        "\n",
        "print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "pill_classifier(args)\n",
        "print('job done')"
      ],
      "metadata": {
        "id": "3K_S3Z1sF3LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **미사용 코드**"
      ],
      "metadata": {
        "id": "-xlBENxkeJ3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **debuger_is_active()**"
      ],
      "metadata": {
        "id": "ngc63B9q02X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import argparse\n",
        "import uuid\n",
        "\n",
        "is_debug = True\n",
        "def debugger_is_active() -> bool:\n",
        "    \"\"\"Return if the debugger is currently active\"\"\"\n",
        "    gettrace = getattr(sys, 'gettrace', lambda : None)\n",
        "    return gettrace() is not None\n",
        "\n",
        "is_debug = debugger_is_active()\n",
        "# print(f'is_debug is {is_debug}')        # debug 모드로 시작하면,  is_debug == True 이다.\n",
        "\n",
        "uuid_node = uuid.getnode()\n"
      ],
      "metadata": {
        "id": "oZaec-snmfHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **copy_crop_pill_from_json**"
      ],
      "metadata": {
        "id": "ODOl0rYH0vjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_crop_pill_from_json(pathfile_src_json, pathfile_src_png, pathfile_dest_json, pathfile_dest_png):\n",
        "\n",
        "    # 이미지 파일이 없으면 함수를 종료\n",
        "    if not pathfile_src_png.exists() :\n",
        "        return\n",
        "\n",
        "    # JSON 파일을 새로운 경로에 복사\n",
        "    shutil.copyfile(str(pathfile_src_json), str(pathfile_dest_json))\n",
        "\n",
        "    # crop\n",
        "    dict_pill_info = utils.read_dict_from_json(str(pathfile_src_json))\n",
        "    bbox = dict_pill_info['annotations'][0]['bbox']\n",
        "    np_bbox = np.array([[bbox[0], bbox[1]], [bbox[0] + bbox[2], bbox[1] + bbox[3]]])\n",
        "    np_center = np.average(np_bbox, axis=0)\n",
        "    diff =  np.max(np_bbox[1] - np_bbox[0])\n",
        "    np_low = np_center - diff/2\n",
        "    np_high = np_center + diff/2\n",
        "    np_high = np_high.astype(np.int32)\n",
        "    np_low = np_low.astype(np.int32)\n",
        "\n",
        "    image_cv = utils.open_opencv_file(str(pathfile_src_png))\n",
        "    image_cv_cropped = image_cv[np_low[1]:np_high[1], np_low[0]:np_high[0]]\n",
        "    image_cv_resized = cv2.resize(image_cv_cropped, (args.size_image, args.size_image))\n",
        "\n",
        "    utils.save_opencv_file(image_cv_resized, str(pathfile_dest_png))\n",
        "\n",
        "json_list_dir_pill_org = '/content/drive/MyDrive/data/1.Training/라벨링데이터/단일경구약제 5000종'\n",
        "data_list_dir_pill_org = '/content/drive/MyDrive/data/1.Training/원천데이터/단일경구약제 5000종/TS_81_단일'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def copy_crop_pill_from_org(list_dir_pill_org):\n",
        "    '''\n",
        "    list_dir_pill_org 의 directory은  알약  dir들을 여러 개 포함하고,\n",
        "    알약 dir에는 조건에 따라, 1200여개의 png가 있다.\n",
        "    이를  아래 pathdir_dest dir에 모든다.\n",
        "    단 이미지를 bbox기준으로, 224x224으로 crop한다.\n",
        "    :param args:\n",
        "    :return:\n",
        "    '''\n",
        "    # 새로운 이미지를 저장할 경로\n",
        "    pathdir_dest = Path(r'/content/drive/MyDrive/data/1.Training/croped_data/')\n",
        "\n",
        "    for dir_pill_org in list_dir_pill_org :\n",
        "        list_count = [ 1 for _ in Path(dir_pill_org).iterdir()]\n",
        "        count_dir = len(list_count)\n",
        "        index_dir = 0\n",
        "        for pathdir_pill_class_json in Path(dir_pill_org).iterdir():\n",
        "            print(f'working dir index is { index_dir}/{count_dir}, {str(pathdir_pill_class_json)}')\n",
        "            index_dir += 1\n",
        "            if not 'json' in pathdir_pill_class_json.name or not pathdir_pill_class_json.is_dir():\n",
        "                continue\n",
        "\n",
        "            no_json = pathdir_pill_class_json.name.split('_')[0]\n",
        "            pathdir_pill_class = pathdir_pill_class_json.with_name(no_json)\n",
        "            pathdir_dest_class = pathdir_dest.joinpath(no_json)\n",
        "            pathdir_dest_class.mkdir(exist_ok=True)\n",
        "\n",
        "            for i,  pathfile_pill_json in enumerate(pathdir_pill_class_json.glob('*.json')):\n",
        "                # if i == 0 :\n",
        "                #     shutil.copy(pathfile_pill_json,pathdir_dest )\n",
        "                pathfile_pill_png = pathdir_pill_class.joinpath( pathfile_pill_json.stem + '.png')\n",
        "                copy_crop_pill_from_json(pathfile_pill_json, pathfile_pill_png, pathdir_dest_class.joinpath(pathfile_pill_json.stem + '.json'), pathdir_dest_class.joinpath(pathfile_pill_json.stem + '.png'))\n"
      ],
      "metadata": {
        "id": "wnISNm7bKy2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **copy_crop_pill_from_org.py**"
      ],
      "metadata": {
        "id": "RBKlWutuLdn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "list_dir_pill_org = [\n",
        "    # r'G:\\경구약제 DB\\0.단일 경구약제 1000종\\경구약제 1000종 추가 DB',\n",
        "    # r'G:\\경구약제 DB\\0.단일 경구약제 1000종\\경구약제 1000종 추가 DB_1',\n",
        "    r'G:\\경구약제 DB\\0.단일 경구약제 1000종\\경구약제 1000종 추가 DB_2',\n",
        "]\n",
        "\n",
        "\n",
        "# JSON 파일과 이미지 파일을 읽어서, 이미지를 자르고, 새로운 크기로 resize 후, 새로운 경로에 저장\n",
        "def copy_crop_pill_from_json(args, pathfile_src_json, pathfile_src_png, pathfile_dest_json, pathfile_edst_png):\n",
        "\n",
        "    # 이미지 파일이 없으면 함수를 종료\n",
        "    if not pathfile_src_png.exists() :\n",
        "        args.logger.info(f\"file does'nt exist : {str(pathfile_src_png)}\")\n",
        "        return\n",
        "\n",
        "    # JSON 파일을 새로운 경로에 복사\n",
        "    shutil.copyfile(str(pathfile_src_json), str(pathfile_dest_json))\n",
        "\n",
        "    # crop\n",
        "    dict_pill_info = utils.read_dict_from_json(str(pathfile_src_json))\n",
        "    bbox = dict_pill_info['annotations'][0]['bbox']\n",
        "    np_bbox = np.array([[bbox[0], bbox[1]], [bbox[0] + bbox[2], bbox[1] + bbox[3]]])\n",
        "    np_center = np.average(np_bbox, axis=0)\n",
        "    diff =  np.max(np_bbox[1] - np_bbox[0])\n",
        "    np_low = np_center - diff/2\n",
        "    np_high = np_center + diff/2\n",
        "    np_high = np_high.astype(np.int32)\n",
        "    np_low = np_low.astype(np.int32)\n",
        "\n",
        "    image_cv = utils.open_opencv_file(str(pathfile_src_png))\n",
        "    image_cv_cropped = image_cv[np_low[1]:np_high[1], np_low[0]:np_high[0]]\n",
        "    image_cv_resized = cv2.resize(image_cv_cropped, (args.size_image, args.size_image))\n",
        "\n",
        "    utils.save_opencv_file(image_cv_resized, str(pathfile_dest_png))\n",
        "\n",
        "# 경로 list_dir_pill_org 에 있는 모든 약제 이미지를 처리\n",
        "def copy_crop_pill_from_org(args):\n",
        "    '''\n",
        "    list_dir_pill_org 의 directory은  알약  dir들을 여러 개 포함하고,\n",
        "    알약 dir에는 조건에 따라, 1200여개의 png가 있다.\n",
        "    이를  아래 pathdir_dest dir에 모든다.\n",
        "    단 이미지를 bbox기준으로, 224x224으로 crop한다.\n",
        "    :param args:\n",
        "    :return:\n",
        "    '''\n",
        "    # 새로운 이미지를 저장할 경로\n",
        "    pathdir_dest = Path(r'G:\\proj_pill_data\\pill_data_croped')\n",
        "\n",
        "    for dir_pill_org in list_dir_pill_org :\n",
        "        list_count = [ 1 for _ in Path(dir_pill_org).iterdir()]\n",
        "        count_dir = len(list_count)\n",
        "        index_dir = 0\n",
        "        for pathdir_pill_class_json in Path(dir_pill_org).iterdir():\n",
        "            print(f'working dir index is { index_dir}/{count_dir}, {str(pathdir_pill_class_json)}')\n",
        "            index_dir += 1\n",
        "            if not 'json' in pathdir_pill_class_json.name or not pathdir_pill_class_json.is_dir():\n",
        "                continue\n",
        "\n",
        "            no_json = pathdir_pill_class_json.name.split('_')[0]\n",
        "            pathdir_pill_class = pathdir_pill_class_json.with_name(no_json)\n",
        "            pathdir_dest_class = pathdir_dest.joinpath(no_json)\n",
        "            pathdir_dest_class.mkdir(exist_ok=True)\n",
        "\n",
        "            for i,  pathfile_pill_json in enumerate(pathdir_pill_class_json.glob('*.json')):\n",
        "                # if i == 0 :\n",
        "                #     shutil.copy(pathfile_pill_json,pathdir_dest )\n",
        "                pathfile_pill_png = pathdir_pill_class.joinpath( pathfile_pill_json.stem + '.png')\n",
        "                copy_crop_pill_from_json(args, pathfile_pill_json, pathfile_pill_png, pathdir_dest_class.joinpath(pathfile_pill_json.stem + '.json'), pathdir_dest_class.joinpath(pathfile_pill_json.stem + '.png'))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     # job = 'hrnet_w64'\n",
        "#     job = 'resnet152'\n",
        "#     args = get_cli_args(job=job, run_phase='train', aug_level=1 )\n",
        "#     args.logger = utils.create_logging(args.file_log)\n",
        "#     copy_crop_pill_from_org(args)\n",
        "#     print('done')"
      ],
      "metadata": {
        "id": "tRe3XiGJLceJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **gen_pill.py**"
      ],
      "metadata": {
        "id": "7_1wJOZWLp-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from imgaug import augmenters as iaa\n",
        "from pathlib import Path\n",
        "import utils\n",
        "import make_label_sharpness\n",
        "from make_pill_class_list import get_pillid_from_pillfile\n",
        "from PIL import Image\n",
        "\n",
        "# https://github.com/aleju/imgaug\n",
        "\n",
        "def rescaleToFit(np_image, np_point, fit_x, fit_y, scale_limit = 0.8):      #  scale_limit is for image rotation with nppoint.\n",
        "    np_image = np_image.copy()\n",
        "    h, w, c = np_image.shape\n",
        "    # show_cvimage(np_image)\n",
        "    scale = fit_x / w       # 먼저 x scale 을 정하고,\n",
        "    if scale * h > fit_y :  # scale이 h 에는  over height.\n",
        "        scale =  fit_y /  h # y scale으로 정한다.\n",
        "\n",
        "    scale = scale * scale_limit\n",
        "    np_image = cv2.resize(np_image, (int(round(w * scale)), int(round(h * scale)) ))\n",
        "    x_scale = np_image.shape[1] / w\n",
        "    y_scale = np_image.shape[0] / h\n",
        "\n",
        "    np_images_back = np.zeros((fit_y, fit_x, c), dtype=np.uint8)\n",
        "    x_offset = (fit_x - np_image.shape[1]) // 2\n",
        "    y_offset = (fit_y - np_image.shape[0]) // 2\n",
        "\n",
        "    np_images_back[y_offset:y_offset + np_image.shape[0], x_offset:x_offset+np_image.shape[1], : ] = np_image\n",
        "\n",
        "\n",
        "    if len(np_point) > 0:\n",
        "        np_point *= (x_scale, y_scale)\n",
        "\n",
        "        np_point[np_point < 0 ] += -1000\n",
        "        np_point += [x_offset, y_offset ]\n",
        "        np_point[np_point < 0 ] = -1\n",
        "\n",
        "    # draw_limbs_on_image(np_images_back,np_point, list_limb_digit0, (0, 0, 255), 2 , False )\n",
        "    # show_cvimage(np_images_back)\n",
        "\n",
        "    return np_images_back, np_point\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "    print(f'done')"
      ],
      "metadata": {
        "id": "a1_KTLBTH5Ue",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "3352b2a4-9a07-44de-ccef-9e6e9c6ce116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a09a8f804c5b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_label_sharpness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmake_pill_class_list\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_pillid_from_pillfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **get_cli_args.py**"
      ],
      "metadata": {
        "id": "bS-C4L-VMAOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import argparse\n",
        "import uuid\n",
        "\n",
        "is_debug = True\n",
        "def debugger_is_active() -> bool:\n",
        "    \"\"\"Return if the debugger is currently active\"\"\"\n",
        "    gettrace = getattr(sys, 'gettrace', lambda : None)\n",
        "    return gettrace() is not None\n",
        "\n",
        "is_debug = debugger_is_active()\n",
        "# print(f'is_debug is {is_debug}')        # debug 모드로 시작하면,  is_debug == True 이다.\n",
        "\n",
        "uuid_node = uuid.getnode()\n"
      ],
      "metadata": {
        "id": "BiKh4MJIJx3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **main_cls0.py**"
      ],
      "metadata": {
        "id": "A2dFtlTuMQdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pill_classifier import *\n",
        "from get_cli_args import get_cli_args\n",
        "\n",
        "\n",
        "job = 'resnet152'\n",
        "if __name__ == '__main__':\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='0')\n",
        "\n",
        "    print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "    end = time.time()\n",
        "    if args.run_phase == 'train'  :\n",
        "        args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "        print(f'train dataset was loaded')\n",
        "\n",
        "    args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "    print(f'valid dataset was loaded')\n",
        "\n",
        "\n",
        "    print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "    pill_classifier(args)\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "6D9wtVOsM3XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **main_cls01_dir.py**"
      ],
      "metadata": {
        "id": "xcKWA3NEMYPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pill_classifier import *\n",
        "from get_cli_args import get_cli_args\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class Dataset_Dir(Dataset):\n",
        "    def __init__(self, args, dir_dataset, transform=None, target_transform=None, run_phase='train'):\n",
        "        self.args = args\n",
        "        self.dir_dataset = dir_dataset\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.list_images = [ png.name  for png in Path(dir_dataset).iterdir() if png.suffix == '.png']\n",
        "        self.run_phase = run_phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.dir_dataset, self.list_images[idx]))\n",
        "        label = 0\n",
        "        path_img = self.list_images[idx]\n",
        "        aug_name = \"\"\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "        if self.run_phase == 'valid' or self.run_phase == 'test':\n",
        "            return image, label, path_img, aug_name\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "\n",
        "job = 'resnet152'\n",
        "if __name__ == '__main__':\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='01')\n",
        "\n",
        "    print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "    dir_testimage = r'.\\dir_testimage'\n",
        "\n",
        "    args.dataset_valid = Dataset_Dir(args, dir_testimage, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "    args.batch_size = len(args.dataset_valid)\n",
        "    args.verbose = False\n",
        "    print(f'valid dataset was loaded')\n",
        "\n",
        "    pill_classifier(args)\n",
        "\n",
        "    print(args.path_img)\n",
        "    print(args.list_preds)\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "i6_O4H18M-b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **main_cls1.py**"
      ],
      "metadata": {
        "id": "Ijg59q82Mbvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pill_classifier import *\n",
        "from get_cli_args import get_cli_args\n",
        "\n",
        "\n",
        "job = 'resnet152'\n",
        "if __name__ == '__main__':\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='1')\n",
        "\n",
        "    print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "    end = time.time()\n",
        "    if args.run_phase == 'train'  :\n",
        "        args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "        print(f'train dataset was loaded')\n",
        "\n",
        "    args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "    print(f'valid dataset was loaded')\n",
        "\n",
        "\n",
        "    print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "    pill_classifier(args)\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "sxbY59RvNF5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **main_cls01.py**"
      ],
      "metadata": {
        "id": "Odu5HEkVMivM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pill_classifier import *\n",
        "from get_cli_args import get_cli_args\n",
        "\n",
        "\n",
        "job = 'resnet152'\n",
        "if __name__ == '__main__':\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='01')\n",
        "\n",
        "    print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "    end = time.time()\n",
        "    if args.run_phase == 'train'  :\n",
        "        args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "        print(f'train dataset was loaded')\n",
        "\n",
        "    args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "    print(f'valid dataset was loaded')\n",
        "\n",
        "\n",
        "    print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "    pill_classifier(args)\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "hLEOmA71NRIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **make_label_sharpness.py**"
      ],
      "metadata": {
        "id": "hzBc2OLvMnpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "from get_cli_args import get_cli_args\n",
        "from pathlib import Path\n",
        "import utils\n",
        "import numpy\n",
        "\n",
        "def estimate_sharpness(image: numpy.array, threshold: int = 100):\n",
        "    if image.ndim == 3:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    blur_map = cv2.Laplacian(image, cv2.CV_64F)\n",
        "    score = numpy.var(blur_map)\n",
        "    return score\n",
        "\n",
        "def make_label_sharpness(args):\n",
        "    pathdir_dest = Path(args.dir_pill_class_base)\n",
        "    dict_pill_prescription_type = utils.read_dict_from_json(args.json_pill_prescription_type)['pill_prescription_type']\n",
        "\n",
        "    list_pills_pres_sharp_score = []\n",
        "    list_pills_otc_sharp_score = []\n",
        "    count = 0\n",
        "    for pathdir_pill_class in pathdir_dest.iterdir():\n",
        "        if not pathdir_pill_class.is_dir() :\n",
        "            continue\n",
        "        print(f'{count}, {pathdir_pill_class} is being estimated')\n",
        "        count += 1\n",
        "        list_pill_class_shape_score = []\n",
        "        for  pathfile_pill_png in pathdir_pill_class.iterdir():\n",
        "            if pathfile_pill_png.suffix == '.json':\n",
        "                continue\n",
        "            score = estimate_sharpness(utils.open_opencv_file(str(pathfile_pill_png)))\n",
        "            list_pill_class_shape_score.append(score)\n",
        "\n",
        "        score_min = min(list_pill_class_shape_score)\n",
        "        score_max = max(list_pill_class_shape_score)\n",
        "        score_mean = sum(list_pill_class_shape_score)/len(list_pill_class_shape_score)\n",
        "        if dict_pill_prescription_type[pathdir_pill_class.stem] == 'PRES':\n",
        "            list_pills_pres_sharp_score.append([str(pathdir_pill_class.stem), score_mean, score_min,score_max])\n",
        "        elif dict_pill_prescription_type[pathdir_pill_class.stem] == 'OTC':\n",
        "            list_pills_otc_sharp_score.append([str(pathdir_pill_class.stem), score_mean, score_min,score_max])\n",
        "\n",
        "    list_pills_pres_sharp_score = sorted(list_pills_pres_sharp_score, key=lambda a: a[2], reverse=True)         # sharp min 기준으로 정렬.\n",
        "    list_pills_otc_sharp_score = sorted(list_pills_otc_sharp_score, key=lambda a: a[2], reverse=True)           # sharp min 기준으로 정렬.\n",
        "    list_pills_label_path_sharp_score = [ [label, pathname, score_mean, score_min, score_max] for label, ( pathname, score_mean, score_min, score_max ) in enumerate(list_pills_pres_sharp_score) if label < 600]\n",
        "    list_pills_label_path_sharp_score += [[label+600, pathname, score_mean, score_min, score_max] for label, (pathname, score_mean, score_min, score_max) in enumerate(list_pills_otc_sharp_score) if label < 400]\n",
        "\n",
        "    dict_temp = { 'pill_label_path_sharp_score':list_pills_label_path_sharp_score}\n",
        "\n",
        "    utils.save_dict_to_json(dict_temp, args.json_pill_label_path_sharp_score)\n",
        "\n",
        "def get_dict_label_pillid(args):\n",
        "    dict_temp = utils.read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_path_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "\n",
        "    dict_label_pillid = {}\n",
        "    for idx , (label, pillid, score_mean, score_min, score_max) in enumerate(list_pills_label_path_sharp_score) :\n",
        "        if idx >= args.num_classes:\n",
        "            break\n",
        "        print(f'{label},{pillid},{score_mean},{score_min},{score_max}')\n",
        "        dict_label_pillid.update({label: pillid})\n",
        "\n",
        "    return dict_label_pillid\n",
        "\n",
        "\n",
        "def get_dict_pillid_label(args):\n",
        "    dict_temp = utils.read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_path_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "\n",
        "    dict_label_pillid = {}\n",
        "    for idx, (label, pillid, score_mean, score_min, score_max) in enumerate(list_pills_label_path_sharp_score):\n",
        "        if idx >= args.num_classes:\n",
        "            break\n",
        "        dict_label_pillid.update({pillid:label})\n",
        "\n",
        "    return dict_label_pillid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='train', aug_level=1, dataclass='0')\n",
        "    args.logger = utils.create_logging(args.file_log)\n",
        "    make_label_sharpness(args)\n",
        "    # get_dict_pillid_label(args)\n",
        "    # dict_label_pillid = get_dict_label_pillid(args)\n",
        "    print('done')\n"
      ],
      "metadata": {
        "id": "nLMJFNf5NSHg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "c9637ad6-dc4e-4ce7-d2ae-7b4b3bdfb3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3dd2f1a8a714>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mget_cli_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cli_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'get_cli_args'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**make_pill_class_list.py**"
      ],
      "metadata": {
        "id": "zOfquQnhMs_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "from get_cli_args import get_cli_args\n",
        "import utils\n",
        "from pathlib import Path\n",
        "import random\n",
        "import os\n",
        "\n",
        "def get_pill_info_from_pillfile(path_png):\n",
        "    pill_basename, pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist = path_png.stem.split('_')\n",
        "    pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist = int(pill_status), int(pill_back), int(pill_front), int(pill_light), int(pill_lati), int(pill_longi), int(pill_dist)\n",
        "    return pill_basename, pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist\n",
        "\n",
        "def get_pillid_from_pillfile(file_png):\n",
        "    try :\n",
        "        pill_basename, pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist = Path(file_png).stem.split('_')\n",
        "    except:\n",
        "        # 상위 dir의  name이  pill id가 된다.\n",
        "        path_file = Path(file_png)\n",
        "        pill_basename = path_file.parts[-2]\n",
        "    return pill_basename\n",
        "\n",
        "def make_pill_class_list(args):\n",
        "    '''\n",
        "    json_pill_sharpness 파일으로 부터, dataset을 만든다.\n",
        "    class 1 :  카메라 위도, 90, 75\n",
        "    class 2 : 카메라 위도 70, 65\n",
        "    :param args:\n",
        "    :return:\n",
        "    train set : 90%\n",
        "    valid set : 10%\n",
        "    test set : 10%\n",
        "    '''\n",
        "\n",
        "    pill_class0 = []\n",
        "    pill_class1 = []\n",
        "    dict_temp = utils.read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_pillid_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "    for label, pillid, score_mean, score_min, score_max in list_pills_label_pillid_sharp_score:\n",
        "        if label >= args.num_classes:\n",
        "            continue\n",
        "        print(f'reading sharp score.  label:{label}')\n",
        "        pillid = Path(os.path.join(args.dir_pill_class_base, pillid))\n",
        "        for file_png in pillid.iterdir():\n",
        "            if file_png.suffix != '.png':\n",
        "                continue\n",
        "\n",
        "            pill_basename, pill_status, pill_back, pill_front, pill_light, pill_lati, pill_longi, pill_dist = get_pill_info_from_pillfile(file_png)\n",
        "\n",
        "            if pill_lati in args.pill_dataset_class0:\n",
        "                pill_class0.append(str(file_png))\n",
        "            elif pill_lati in args.pill_dataset_class1:\n",
        "                pill_class1.append(str(file_png))\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    list_index_class0 = list(range(len(pill_class0)))\n",
        "    len_train = int(round(args.pill_dataset_train_rate * len(list_index_class0)))\n",
        "    list_index_class0_train = random.sample(list_index_class0, len_train)\n",
        "    list_index_class0_valid = list(set(list_index_class0) - set(list_index_class0_train))\n",
        "    list_index_class0_test = random.sample(list_index_class0_valid, int(round(len(list_index_class0_valid) * (args.pill_dataset_test_rate / (args.pill_dataset_test_rate + args.pill_dataset_valid_rate)))))\n",
        "    list_index_class0_valid = list(set(list_index_class0_valid) - set(list_index_class0_test))\n",
        "\n",
        "    list_index_class1 = list(range(len(pill_class1)))\n",
        "    len_train = int(round(args.pill_dataset_train_rate * len(list_index_class1)))\n",
        "    list_index_class1_train = random.sample(list_index_class1, len_train)\n",
        "    list_index_class1_valid = list(set(list_index_class1) - set(list_index_class1_train))\n",
        "    list_index_class1_test = random.sample(list_index_class1_valid, int(round(len(list_index_class1_valid) * (args.pill_dataset_test_rate / (args.pill_dataset_test_rate + args.pill_dataset_valid_rate)))))\n",
        "    list_index_class1_valid = list(set(list_index_class1_valid) - set(list_index_class1_test))\n",
        "\n",
        "    list_pngfile_class0_train = [pill_class0[index] for index in list_index_class0_train]\n",
        "    list_pngfile_class0_valid = [pill_class0[index] for index in list_index_class0_valid]\n",
        "    list_pngfile_class0_test = [pill_class0[index] for index in list_index_class0_test]\n",
        "\n",
        "    list_pngfile_class1_train = [pill_class1[index] for index in list_index_class1_train]\n",
        "    list_pngfile_class1_valid = [pill_class1[index] for index in list_index_class1_valid]\n",
        "    list_pngfile_class1_test = [pill_class1[index] for index in list_index_class1_test]\n",
        "\n",
        "    print(f'pngfile_class0_train:{len(list_pngfile_class0_train)}')\n",
        "    print(f'pngfile_class0_valid:{len(list_pngfile_class0_valid)}')\n",
        "    print(f'pngfile_class0_test:{len(list_pngfile_class0_test)}')\n",
        "\n",
        "    print(f'pngfile_class1_train:{len(list_pngfile_class1_train)}')\n",
        "    print(f'pngfile_class1_valid:{len(list_pngfile_class1_valid)}')\n",
        "    print(f'pngfile_class1_test:{len(list_pngfile_class1_test)}')\n",
        "\n",
        "    dict_temp = {'pngfile_class0_train': list_pngfile_class0_train, 'pngfile_class0_valid': list_pngfile_class0_valid, 'pngfile_class0_test': list_pngfile_class0_test, 'pngfile_class1_train': list_pngfile_class1_train, 'pngfile_class1_valid': list_pngfile_class1_valid, 'pngfile_class1_test': list_pngfile_class1_test, }\n",
        "\n",
        "    utils.save_dict_to_json(dict_temp, args.json_pill_class_list)\n",
        "\n",
        "def rename_non_candidate_to_s_id(args):\n",
        "    # 후보가 아닌 알약 directory을  다른 이름(K head)으로 변경한다.\n",
        "    dict_temp = utils.read_dict_from_json(args.json_pill_label_path_sharp_score)\n",
        "    list_pills_label_pillid_sharp_score = dict_temp['pill_label_path_sharp_score']\n",
        "    list_candidate_ids = [ pillid for label, pillid, score_mean, score_min, score_max in list_pills_label_pillid_sharp_score ]\n",
        "\n",
        "    path_pill_base = Path(args.dir_pill_class_base )\n",
        "    list_pill_all_id = []\n",
        "    for pill_dir in path_pill_base.iterdir():\n",
        "        if not pill_dir.is_dir() :\n",
        "            continue\n",
        "        list_pill_all_id.append(pill_dir.stem)\n",
        "\n",
        "    list_non_candidate_pillid = list( set(list_pill_all_id) - set(list_candidate_ids))\n",
        "\n",
        "    for pillid in list_non_candidate_pillid :\n",
        "        new_id = pillid.replace('K', 'S')\n",
        "        path_old = os.path.join(args.dir_pill_class_base, pillid)\n",
        "        path_new = os.path.join(args.dir_pill_class_base, new_id)\n",
        "        shutil.move(path_old, path_new)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='train', aug_level=1, dataclass='0')\n",
        "    args.logger = utils.create_logging(args.file_log)\n",
        "    # make_pill_class_list(args)\n",
        "    rename_non_candidate_to_s_id(args)\n",
        "    print('job done')\n"
      ],
      "metadata": {
        "id": "wQ96F8I6K53M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**pill_classifier_hvd.py**"
      ],
      "metadata": {
        "id": "UZol2gyfMyED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from get_cli_args import get_cli_args\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from hrnet import get_hrnet\n",
        "from utils import model_load, transform_normalize, get_optimizer\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as mp\n",
        "from pill_classifier import get_pill_model, Dataset_Pill, run_model\n",
        "import horovod.torch as hvd\n",
        "\n",
        "\n",
        "def adjust_learning_rate_hvd(args, optimizer, len_loader, epoch, batch_idx):\n",
        "    if epoch < args.warmup_epochs:\n",
        "        epoch += float(batch_idx + 1) / len_loader\n",
        "        lr_adj = 1. / hvd.size() * (epoch * (hvd.size() - 1) / args.warmup_epochs + 1)\n",
        "    elif epoch < 30:\n",
        "        lr_adj = 1.\n",
        "    elif epoch < 60:\n",
        "        lr_adj = 1e-1\n",
        "    elif epoch < 80:\n",
        "        lr_adj = 1e-2\n",
        "    else:\n",
        "        lr_adj = 1e-3\n",
        "\n",
        "    lr = args.base_lr * hvd.size() * args.batches_per_allreduce * lr_adj\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    return lr\n",
        "\n",
        "def pill_classifier_hvd(args):\n",
        "    # pytorch의 DataLoader 함수를 사용할 때 num_workers>1인 경우를  지원해서, fork가 지원되게.\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print(f'model path is {args.model_path_in}')\n",
        "\n",
        "    args.allreduce_batch_size = args.batch_size * args.batches_per_allreduce\n",
        "\n",
        "    hvd.init()\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    if args.cuda:\n",
        "        # Horovod: pin GPU to local rank.\n",
        "        torch.cuda.set_device(hvd.local_rank())\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "        args.gpu = hvd.local_rank()\n",
        "\n",
        "    cudnn.benchmark = True  # 내장된 cudnn 자동 튜너를 활성화하여, 하드웨어에 맞게 사용할 최상의 알고리즘(텐서 크기나 conv 연산에 맞게?)을 찾는다.\n",
        "                            # 입력 이미지 크기가 자주 변하지 않는다면, 초기 시간이 소요되지만 일반적으로 더 빠른 런타임의 효과를 볼 수 있다\n",
        "    torch.backends.cudnn.deterministic = False      # 만일 True이면, cudnn에 맞추는 알고리즘이 없으면, Raise Error가 된다.\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    verbose = args.verbose if hvd.rank() == 0 else False\n",
        "    args.rank = hvd.rank()\n",
        "\n",
        "    # Horovod: write TensorBoard logs on first worker.\n",
        "    log_writer = SummaryWriter(args.dir_log) if hvd.rank() == 0 else None\n",
        "\n",
        "    # Horovod: limit # of CPU threads to be used per worker.\n",
        "    torch.set_num_threads(args.num_threads)\n",
        "\n",
        "    kwargs = {'num_workers': args.num_workers, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "    # When supported, use 'forkserver' to spawn dataloader workers instead of 'fork' to prevent\n",
        "    # issues with Infiniband implementations that are not fork-safe\n",
        "    if (kwargs.get('num_workers', 0) > 0 and hasattr(mp, '_supports_context') and\n",
        "        mp._supports_context and 'forkserver' in mp.get_all_start_methods()):\n",
        "        kwargs['multiprocessing_context'] = 'forkserver'\n",
        "\n",
        "    #############################################################################\n",
        "    print(\"Loading dataset...\")\n",
        "\n",
        "    if args.run_phase == 'train':\n",
        "        dataset_train = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='train')\n",
        "        sampler_train = torch.utils.data.distributed.DistributedSampler(dataset_train, num_replicas=hvd.size(), rank=hvd.rank())\n",
        "        dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, sampler=sampler_train, **kwargs)\n",
        "\n",
        "    dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='valid')\n",
        "    sampler_valid = torch.utils.data.distributed.DistributedSampler(dataset_valid, num_replicas=hvd.size(), rank=hvd.rank())\n",
        "    dataloader_valid = DataLoader(dataset_valid, batch_size=args.batch_size, sampler=sampler_valid, **kwargs)\n",
        "\n",
        "\n",
        "    model = get_pill_model(args)\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "    optimizer = get_optimizer(args, model)\n",
        "    compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none\n",
        "\n",
        "    optimizer = hvd.DistributedOptimizer(\n",
        "        optimizer, named_parameters=model.named_parameters(),\n",
        "        compression=compression,\n",
        "        backward_passes_per_step=args.batches_per_allreduce,\n",
        "        op=hvd.Adasum if args.use_adasum else hvd.Average,\n",
        "        gradient_predivide_factor=args.gradient_predivide_factor)\n",
        "\n",
        "    epoch_begin, dict_checkpoint, success = model_load(args, model, optimizer)\n",
        "    epoch_begin = hvd.broadcast(torch.tensor(epoch_begin), root_rank=0, name='epoch_begin').item()\n",
        "\n",
        "    # Horovod: broadcast parameters & optimizer state.\n",
        "    hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
        "    hvd.broadcast_optimizer_state(optimizer, root_rank=0)           # optimizer.state_dict()가 아니다. optimizer가 들어간다.\n",
        "\n",
        "\n",
        "    run_model(args, model, dataloader_train, dataloader_valid, sampler_train, sampler_valid, criterion,optimizer, epoch_begin, log_writer, verbose  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='train', aug_level=0, dataclass='0')\n",
        "    pill_classifier_hvd(args)\n",
        "\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "RnET5Dh4M0yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**pill_classifier.py**"
      ],
      "metadata": {
        "id": "sCao1a5fM2h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from get_cli_args import get_cli_args\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from gen_pill import Gen_Digit\n",
        "from hrnet import get_hrnet\n",
        "from utils import model_load, model_save, accuracy, get_optimizer, transform_normalize, AverageMeter, adjust_learning_rate\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "\n",
        "class Dataset_Pill(Dataset):\n",
        "    def __init__(self, args, dir_dataset, transform=None, target_transform=None, run_phase='train'):\n",
        "        self.args = args\n",
        "        self.gen_digit = Gen_Digit(args, dir_dataset, run_phase)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.run_phase = run_phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.gen_digit.len_total\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label, path_img, aug_name = self.gen_digit.generate_digits_by_index(self.args, idx)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "        if self.run_phase == 'valid' or self.run_phase == 'test':\n",
        "            return image, label, path_img, aug_name\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "def get_pill_model(args):\n",
        "\n",
        "    if args.cnn_name == 'resnet152' :\n",
        "        model = models.resnet152(num_classes=args.num_classes)\n",
        "    elif args.cnn_name == 'hrnet_w64' :\n",
        "        model = get_hrnet()\n",
        "        model.classifier = nn.Linear(in_features=2048, out_features=args.num_classes, bias=True)\n",
        "    else:\n",
        "        raise Exception('No Found CNN Name')\n",
        "\n",
        "    if args.cuda == True:\n",
        "        if args.gpu is not None:\n",
        "            model.cuda(args.gpu)\n",
        "        else:\n",
        "            model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def train(args, dataloader, sampler, model, criterion, optimizer, epoch, log_writer=None, verbose=True):\n",
        "    model.train()\n",
        "    if sampler != None:\n",
        "        sampler.set_epoch(epoch)\n",
        "\n",
        "    metric_train_loss = AverageMeter()\n",
        "    ametric_data_time = AverageMeter()\n",
        "\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    with tqdm(total=len(dataloader), desc=args.tqdm_desc_head + 'Train Epoch  #{}'.format(epoch), disable=not verbose) as t:\n",
        "        for batch_idx, (img, target) in enumerate(dataloader):\n",
        "            if args.cuda:\n",
        "                img = img.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(img)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            prec1, prec5 = accuracy(output, target, (1, 5))\n",
        "            count_try = img.cpu().shape[0]\n",
        "            top1.update(prec1[0].detach().cpu().item(), count_try)\n",
        "            top5.update(prec5[0].detach().cpu().item(), count_try)\n",
        "\n",
        "            metric_train_loss.update(loss.detach().cpu().item(), count_try)\n",
        "            t.set_postfix({'loss': metric_train_loss.avg, 'lr':lr,  'top1':top1.avg, 'top5':top5.avg })\n",
        "            t.update(1)\n",
        "\n",
        "    ametric_data_time.update(time.time() - end)\n",
        "\n",
        "    if log_writer:\n",
        "        log_writer.add_scalar('train/loss', metric_train_loss.avg, epoch)\n",
        "\n",
        "    try:\n",
        "        print_string = 'Epoch: [{0}][{1}/{2}]\\t'.format(epoch, batch_idx, len(dataloader))\n",
        "        print_string += 'Data time {data_time.val:.3f} ({data_time.avg:.3f} Now:{Now})\\t'.format(data_time=ametric_data_time, Now=datetime.datetime.now())\n",
        "        print_string += 'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=metric_train_loss)\n",
        "        print_string += 'Accuracy top1:{top1.avg:.4f}, top5:{top5.avg:.4f}'.format(top1=top1, top5=top5)\n",
        "        print(print_string)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return metric_train_loss.avg\n",
        "\n",
        "\n",
        "def valid(args, dataloader, sampler,  model, criterion, epoch, log_writer=None, verbose=True):\n",
        "    metric_train_loss = AverageMeter()\n",
        "    ametric_data_time = AverageMeter()\n",
        "\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "    if sampler != None:\n",
        "        sampler.set_epoch(epoch)\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    args.list_preds = []\n",
        "    args.list_target = []\n",
        "    args.count_correct = 0\n",
        "    with torch.no_grad():\n",
        "        with tqdm(total=len(dataloader), desc=args.tqdm_desc_head + '{} Epoch  #{}'.format( args.run_phase, epoch), disable=not verbose) as t:\n",
        "            for i, (img, target, path_img, aug_name ) in enumerate(dataloader):\n",
        "                # measure data loading time\n",
        "\n",
        "                if args.cuda:\n",
        "                    img = img.cuda()\n",
        "                    target = target.cuda()\n",
        "\n",
        "                output = model(img)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                prec1, prec5 = accuracy(output, target, (1, 5))\n",
        "\n",
        "                if ( prec1[0].detach().cpu().item() != 100.):\n",
        "                    if args.run_phase == 'valid': print(f'<------- class valid fail file: {path_img[0]}, aug_name:{aug_name}')\n",
        "\n",
        "                preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "                count_correct = preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "                list_preds = preds.view(-1).tolist()\n",
        "\n",
        "                args.list_preds = list_preds\n",
        "                args.list_target = target.detach().cpu().tolist()\n",
        "                args.count_correct = count_correct.item()\n",
        "                count_try = img.cpu().shape[0]\n",
        "                top1.update(prec1[0].detach().cpu().item(), count_try)\n",
        "                top5.update(prec5[0].detach().cpu().item(), count_try)\n",
        "\n",
        "                metric_train_loss.update(loss.detach().cpu().item(), count_try)\n",
        "\n",
        "                t.set_postfix({'loss': metric_train_loss.avg, 'top1': top1.avg, 'top5': top5.avg})\n",
        "                t.update(1)\n",
        "\n",
        "    ametric_data_time.update(time.time() - end)\n",
        "\n",
        "    if log_writer:\n",
        "        log_writer.add_scalar('validation/loss', metric_train_loss.avg, epoch)\n",
        "\n",
        "    try:\n",
        "        print_string = 'Epoch: [{0}][{1}/{2}]\\t'.format(epoch, i, len(dataloader))\n",
        "        print_string += 'Data time {data_time.val:.3f} ({data_time.avg:.3f} Now:{Now})\\t'.format(data_time=ametric_data_time, Now=datetime.datetime.now())\n",
        "        print_string += 'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(loss=metric_train_loss)\n",
        "        print_string += 'Accuracy top1:{top1.avg:.4f}, top5:{top5.avg:.4f}'.format(top1=top1, top5=top5)\n",
        "        print(print_string)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return metric_train_loss.avg\n",
        "\n",
        "def run_model(args, model, dataloader_train, dataloader_valid, sampler_train, sampler_valid, criterion,optimizer, epoch_begin, log_writer, verbose=True  ):\n",
        "    if args.run_phase == 'valid' or args.run_phase == 'test':\n",
        "        print(time.asctime())\n",
        "        valid(args, dataloader_valid, sampler_valid,  model, criterion, 0, log_writer)\n",
        "        print(time.asctime())\n",
        "        return\n",
        "\n",
        "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=3, min_lr=0, eps=1e-08)\n",
        "\n",
        "\n",
        "    best_perf = 1000\n",
        "    for epoch in range(epoch_begin, args.epochs):   # ( 0:100)\n",
        "        # adjust_learning_rate(args, optimizer, epoch)\n",
        "        # train for one epoch\n",
        "        perf_indicator = train(args, dataloader_train, sampler_train, model, criterion, optimizer, epoch, log_writer, verbose)\n",
        "        if epoch > 10:\n",
        "            perf_indicator = valid(args, dataloader_valid,sampler_valid, model, criterion, epoch, log_writer,verbose)\n",
        "            if (args.gpu == 0):\n",
        "                print(f'perf_indicator:{perf_indicator} ,  best_perf:{best_perf}')\n",
        "            if perf_indicator < best_perf:\n",
        "                model_save(args.model_path, epoch, model, optimizer, args.rank)\n",
        "                best_perf = perf_indicator\n",
        "        else:\n",
        "            model_save(args.model_path, epoch, model, optimizer, args.rank)\n",
        "            best_perf = perf_indicator\n",
        "\n",
        "        lr_scheduler.step(perf_indicator)\n",
        "\n",
        "\n",
        "model = None\n",
        "criterion = None\n",
        "optimizer = None\n",
        "epoch_begin = 0\n",
        "log_writer = None\n",
        "\n",
        "def pill_classifier(args):\n",
        "    global model, criterion, optimizer, epoch_begin, log_writer\n",
        "    if args.dataset_valid != None:\n",
        "        dataloader_valid = DataLoader(args.dataset_valid, batch_size=args.batch_size, shuffle= False, num_workers=args.num_workers)\n",
        "    else:\n",
        "        dataloader_valid = None\n",
        "\n",
        "    if args.run_phase == 'train' and args.dataset_train != None:\n",
        "        dataloader_train = DataLoader(args.dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "    else:\n",
        "        dataloader_train = None\n",
        "\n",
        "    if model == None :\n",
        "        log_writer = SummaryWriter(args.dir_log)\n",
        "        if args.cuda == False or torch.cuda.device_count() == 0  :\n",
        "            args.gpu = None\n",
        "        else:\n",
        "            args.gpu = 0\n",
        "\n",
        "        args.rank = args.gpu\n",
        "\n",
        "        cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "        model = get_pill_model(args)\n",
        "\n",
        "        # define loss function (criterion) and optimizer\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        if args.cuda:\n",
        "            criterion = criterion.cuda()\n",
        "        optimizer = get_optimizer(args,model)\n",
        "        epoch_begin, dict_checkpoint, success = model_load(args, model, optimizer)\n",
        "\n",
        "    run_model(args, model, dataloader_train, dataloader_valid, None, None,  criterion,optimizer, epoch_begin, log_writer )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # job = 'hrnet_w64'\n",
        "    job = 'resnet152'\n",
        "    args = get_cli_args(job=job, run_phase='test', aug_level=0, dataclass='0')\n",
        "\n",
        "    print(f'model_path_in is {args.model_path_in}')\n",
        "\n",
        "    end = time.time()\n",
        "    if args.run_phase == 'train'  :\n",
        "        args.dataset_train = Dataset_Pill(args, args.json_pill_class_list,  transform=transform_normalize, run_phase='train')\n",
        "        print(f'train dataset was loaded')\n",
        "\n",
        "    args.dataset_valid = Dataset_Pill(args, args.json_pill_class_list, transform=transform_normalize, run_phase='test' if args.run_phase == 'test' else 'valid')\n",
        "    print(f'valid dataset was loaded')\n",
        "\n",
        "    print(f'dataset loading time is {time.time() - end}')\n",
        "\n",
        "    pill_classifier(args)\n",
        "    print('job done')"
      ],
      "metadata": {
        "id": "C7Yeo6-VM5us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**utils.py**"
      ],
      "metadata": {
        "id": "5NTh7yaEM-E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs, json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import logging\n",
        "\n",
        "\n",
        "def inverse_vgg_preprocess(image):\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "    image = image.copy().transpose((1, 2, 0))  # 원본 손상 방지.\n",
        "\n",
        "    for i in range(3):\n",
        "        image[:, :, i] = image[:, :, i] * stds[i]\n",
        "        image[:, :, i] = image[:, :, i] + means[i]\n",
        "    image = image[:, :, ::-1]\n",
        "    image = image * 255\n",
        "\n",
        "    image[image > 255.] = 255.\n",
        "    image[image < 0.] = 0.\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    return image\n",
        "\n",
        "##########################################################################\n",
        "def save_dict_to_json(dict_save, filejson, mode='w'):\n",
        "    with codecs.open(filejson, mode, encoding='utf-8') as f:\n",
        "        json.dump(dict_save, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "def read_dict_from_json(filejson):\n",
        "    if not os.path.isfile(filejson) :\n",
        "        return None\n",
        "    with codecs.open(filejson, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        return obj\n",
        "\n",
        "def open_opencv_file(filename):\n",
        "    img_array = np.fromfile(filename, np.uint8)\n",
        "    image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "\n",
        "def save_opencv_file(image, filename):\n",
        "    result, encoded_img = cv2.imencode('.jpg', image)\n",
        "    if result:\n",
        "        with open(filename, mode='w+b') as f:\n",
        "            encoded_img.tofile(f)\n",
        "            return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def show_cvimage(image):\n",
        "    cv2.imshow('a', image)\n",
        "    cv2.waitKey()                   # wait을 해야, 이미지가 나온다. PIL은  im.show()으로 바로 나온다.\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def show_tensor3(inp, cmap=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp, cmap)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_tensor3(inp, filename):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    matplotlib.image.imsave(filename, inp)\n",
        "\n",
        "\n",
        "def model_save(model_path, epoch, model, optimizer, rank=0) :\n",
        "    if rank == 0 :\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict() if optimizer != None else 0 ,\n",
        "        }, model_path)\n",
        "        print(f'model was saved to {model_path}')\n",
        "\n",
        "def model_load(args, model, optimizer, rank=0) :\n",
        "    epoch_begin = 0\n",
        "    state_model = None\n",
        "    state_optimizer = None\n",
        "    if not os.path.isfile(args.model_path_in):\n",
        "        print(f\"-------------------------------------------------->>>>>>>>>>>>> model_path doesn't exist:{args.model_path_in}\")\n",
        "        return  epoch_begin, None, False      # None check point to indicate a fail\n",
        "\n",
        "    if args.verbose == True: print(f\"model_path will be loaded from:{args.model_path_in}\")\n",
        "\n",
        "    dict_checkpoint = torch.load(args.model_path_in, map_location='cpu')\n",
        "\n",
        "    epoch_begin = dict_checkpoint.get('epoch', -1 ) + 1\n",
        "    if rank == 0 :\n",
        "        try:\n",
        "            state_model = dict_checkpoint.get('model', None )\n",
        "            if state_model != None :\n",
        "                if not hasattr(model, 'module') and ('module' in list(state_model.keys())[0] ) :           # model_path은  module 을 포함하고 있다고 가정한다.\n",
        "                    state_model = OrderedDict([ ( k[7:], v ) if 'module' in k else (k,v ) for k,v in state_model.items()])\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'model was loaded from module state')\n",
        "                elif hasattr(model, 'module') and (not 'module' in list(state_model.keys())[0] ) :\n",
        "                    state_model = OrderedDict([('module.' + k, v)  for k, v in state_model.items()])\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'module.model was loaded from normal state')\n",
        "                else:\n",
        "                    model.load_state_dict(state_model)\n",
        "                    print(f'model was loaded from state')\n",
        "            else:\n",
        "                print(f'No model checkpoint in file')\n",
        "\n",
        "        except Exception as e :\n",
        "            print(f'Fail to loading model: {e}')\n",
        "            return epoch_begin, dict_checkpoint, False\n",
        "\n",
        "        if optimizer != None and args.run_phase == 'train':\n",
        "            try:\n",
        "                state_optimizer = dict_checkpoint.get('optimizer', None )\n",
        "                if state_optimizer != None:\n",
        "                    optimizer.load_state_dict(state_optimizer)\n",
        "                    print(f'optimizer was loaded from state')\n",
        "                else:\n",
        "                    print(f'No optimizer checkpoint in file')\n",
        "            except Exception as e :\n",
        "                print(f'Fail to loading optimizer: {e}')\n",
        "                return epoch_begin, dict_checkpoint, False\n",
        "    success = True if state_model != None and state_optimizer != None else False\n",
        "    return epoch_begin, dict_checkpoint, success\n",
        "\n",
        "\n",
        "def convert_pil_to_cv2(pil_image):\n",
        "    pil_image = pil_image.convert('RGB')\n",
        "    open_cv_image = np.array(pil_image)\n",
        "    # Convert RGB to BGR\n",
        "    open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
        "    return open_cv_image\n",
        "\n",
        "def convert_cv2_to_pil(cv_image):\n",
        "    pil_img = Image.fromarray(cv_image)\n",
        "    return pil_img\n",
        "\n",
        "def save_img_paf_heat(path, img_temp, paf_temp, heatmap_temp, args ):\n",
        "    path = Path(path)\n",
        "\n",
        "    if path.parts[-2].isdigit() :\n",
        "        dir_paf = os.path.join(args.dir_review_paf_train, path.parts[-2] )\n",
        "        dir_heat = os.path.join(args.dir_review_heat_train, path.parts[-2])\n",
        "        dir_img = os.path.join(args.dir_review_img_train, path.parts[-2])\n",
        "    else:\n",
        "        dir_paf = args.dir_output\n",
        "        dir_heat = args.dir_output\n",
        "        dir_img = args.dir_output\n",
        "\n",
        "    os.makedirs(dir_paf, exist_ok=True)\n",
        "    os.makedirs(dir_heat, exist_ok=True)\n",
        "    os.makedirs(dir_img, exist_ok=True)\n",
        "\n",
        "    file_base = path.name.split('.')[0]\n",
        "    filename_img = os.path.join(dir_img, file_base + '_base.jpg')\n",
        "    filename_paf = os.path.join(dir_paf, file_base + '_paf.jpg')\n",
        "    filename_heat = os.path.join(dir_heat, file_base + '_heat.jpg')\n",
        "\n",
        "    save_opencv_file(inverse_vgg_preprocess(img_temp), filename_img)\n",
        "    save_opencv_file(inverse_vgg_preprocess(paf_temp), filename_paf)\n",
        "    save_opencv_file(inverse_vgg_preprocess(heatmap_temp), filename_heat)\n",
        "\n",
        "def open_pil_as_stack_gray_np(filename):\n",
        "    np_pil = np.array(Image.open(filename).convert('L'))\n",
        "    np_pil = np.dstack([np_pil,np_pil,np_pil])\n",
        "    return np_pil\n",
        "\n",
        "def open_pil_as_stack_color_np(filename):\n",
        "    np_pil = np.array(Image.open(filename))\n",
        "    return np_pil\n",
        "\n",
        "def save_np_pil_file(np_image, filename):\n",
        "    image_pil = Image.fromarray(np_image)\n",
        "    image_pil.save(filename)\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)      #  maxk, dim=1, largest, sorted -> ( value tensor, index longTensor )\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "def saveimage(sub, dir_digit, basename):\n",
        "    filename_jpg = os.path.join(dir_digit, basename + '.jpg')\n",
        "    sub.save(filename_jpg)\n",
        "\n",
        "\n",
        "def extractDigit_saveto(file_json, file_bmp, list_dir_digit=None):\n",
        "    dict_bmp_info = read_dict_from_json(file_json)\n",
        "\n",
        "    digitFractNo = int(dict_bmp_info['digitFractNo'])\n",
        "    digitAllNo = int(dict_bmp_info['digitAllNo'])\n",
        "    dataValue = int(dict_bmp_info['dataValue'] * 10 ** digitFractNo)\n",
        "    digitRect = dict_bmp_info['digitRect']\n",
        "    str_dataValue = f'{dataValue:0{digitAllNo}}'\n",
        "    str_igmsGaugeDataId = dict_bmp_info['igmsGaugeDataId']\n",
        "\n",
        "    if len(str_dataValue) != digitAllNo:\n",
        "        if len(str_igmsGaugeDataId) == digitAllNo:\n",
        "            str_dataValue = str_igmsGaugeDataId\n",
        "        else:\n",
        "            print(f'{file_json}')\n",
        "            raise Exception(\"improper data format\")\n",
        "\n",
        "\n",
        "    list_digitRect = digitRect.split('|')[1:digitAllNo+1]\n",
        "    list_digitRect = [aa.split(',') for aa in list_digitRect]\n",
        "    list_digitRect = [[int(a), int(b), int(c), int(d)] for a, b, c, d in list_digitRect]\n",
        "\n",
        "    img = Image.open(file_bmp)\n",
        "    if img == None:\n",
        "        print(f\"Can't read a image file :{file_bmp}\")\n",
        "        return\n",
        "\n",
        "    list_image = []\n",
        "    for index in range(digitAllNo):\n",
        "        x, y, width, height = list_digitRect[index]\n",
        "        sub = img.crop((x, y, x + width, y + height))\n",
        "        if list_dir_digit != None :\n",
        "            saveimage(sub, list_dir_digit[int(str_dataValue[index])], os.path.basename(file_json).split('.')[0] + f'_{index}{str_dataValue[index]}')\n",
        "        else:\n",
        "            list_image.append(sub.convert('RGB'))\n",
        "\n",
        "    if list_dir_digit == None :\n",
        "        return list_image, [int(aa) for aa in str_dataValue], dict_bmp_info\n",
        "\n",
        "def get_Image_Value_List_from_json(file_json):\n",
        "    list_image, list_value, dict_json_info = extractDigit_saveto(file_json, os.path.splitext(file_json)[0] + '.bmp')\n",
        "    list_cv_label_path = [(list_image[i], list_value[i], file_json) for i in range(len(list_image))]\n",
        "    return list_cv_label_path\n",
        "\n",
        "class Dataset_valid(Dataset):\n",
        "    def __init__(self, file_json, transform):\n",
        "        if os.path.isfile(file_json) and file_json.split('.')[-1] == 'json':\n",
        "            self.list_cv_label_path = get_Image_Value_List_from_json(file_json)\n",
        "        elif os.path.isdir(file_json):\n",
        "            self.list_cv_label_path = [(Image.open(aa).convert(\"RGB\"), int(str(Path(aa).parts[-2])), aa) for aa in glob(file_json + r'/**/*.jpg')]\n",
        "        self.transform = transform\n",
        "        self.file_json = file_json\n",
        "\n",
        "    def __len__(self):\n",
        "        # return size of dataset\n",
        "        return len(self.list_cv_label_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label, path = self.list_cv_label_path[idx]\n",
        "        if self.transform != None:\n",
        "            image = self.transform(image)\n",
        "        return image, (label, path)\n",
        "\n",
        "def get_optimizer(args, model):\n",
        "    optimizer = None\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.wd,\n",
        "            nesterov=args.nesterov\n",
        "        )\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:sgd')\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = optim.Adam( filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:adam')\n",
        "    elif args.optimizer == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=args.lr,\n",
        "            momentum=args.momentum,\n",
        "            weight_decay=args.wd,\n",
        "        )\n",
        "        if args.verbose == True: print(f'optimizer was selected as type:rmsprop')\n",
        "    return optimizer\n",
        "\n",
        "def adjust_learning_rate(args, optimizer, epoch):\n",
        "    assert hasattr(args, 'lr_schedule'), \"args doesn't have lr schedule\"\n",
        "    assert hasattr(args, 'lr_gamma'), \"args doesn't have lr gamma\"\n",
        "    assert hasattr(args, 'lr'), \"args doesn't have lr\"\n",
        "\n",
        "    if epoch in args.lr_schedule:\n",
        "        args.lr *= args.lr_gamma\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = args.lr\n",
        "\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"  val을 계속 누적하고,  평균을 구함.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.sum / self.count\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self.sum\n",
        "\n",
        "def create_logging(file_log):\n",
        "    path_file_log = Path(file_log)\n",
        "    path_dir_log = path_file_log.parent\n",
        "    path_dir_log.mkdir(exist_ok=True)\n",
        "\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    formatter_stream = logging.Formatter(u'%(message)s')\n",
        "    streamingHandler = logging.StreamHandler()\n",
        "    streamingHandler.setFormatter(formatter_stream)\n",
        "\n",
        "    formatter_file = logging.Formatter(u'%(asctime)s [%(levelname)8s] %(message)s')\n",
        "    file_handler = logging.FileHandler(file_log)\n",
        "    file_handler.setFormatter(formatter_file)\n",
        "\n",
        "    logger.addHandler(streamingHandler)\n",
        "    logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "\n"
      ],
      "metadata": {
        "id": "4TRPtBR5NAZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}